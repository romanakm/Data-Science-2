{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa8c2ec",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "In this tutorial we learn how the most basic neural networks work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d850a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fd7c84",
   "metadata": {},
   "source": [
    "# Training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abb2a55",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "Create toy dataset, define class for weights intialization etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb7e869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTS OF THE MODEL \n",
      "\n",
      "Hidden layer weights: \n",
      "[[ 2  1  1]\n",
      " [ 1  0 -1]] \n",
      "\n",
      "Hidden layer bias: \n",
      "[[1 0 2]] \n",
      "\n",
      "Output layer weights: \n",
      "[[ 1]\n",
      " [ 1]\n",
      " [-2]] \n",
      "\n",
      "Hidden layer bias: \n",
      "1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set the true weights, use them for data generation later:\n",
    "w_1 = np.array(np.transpose([[2, 1], [1, 0], [1, -1]]))\n",
    "b_1 = np.array(np.expand_dims([1, 0, 2], axis=0))\n",
    "w_2 = np.array([[1,], [1,], [-2,]])\n",
    "b_2 = 1\n",
    "\n",
    "w_1.shape, b_1.shape, w_2.shape\n",
    "\n",
    "# Print the ouput nicely\n",
    "print(\"WEIGHTS OF THE MODEL \\n\")\n",
    "print(\"Hidden layer weights: \\n{} \\n\".format(w_1))\n",
    "print(\"Hidden layer bias: \\n{} \\n\".format(b_1))\n",
    "print(\"Output layer weights: \\n{} \\n\".format(w_2))\n",
    "print(\"Hidden layer bias: \\n{} \\n\".format(b_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "782efc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class for initialization of weights - fix seed to use the same numbers always\n",
    "class MyInit(tf.keras.initializers.Initializer):\n",
    "\n",
    "  def __init__(self, mean, std):\n",
    "    self.mean = mean\n",
    "    self.std = std\n",
    "\n",
    "  def __call__(self, shape, dtype=None, **kwargs):\n",
    "    tf.random.set_seed(11)\n",
    "    return tf.cast(tf.cast(tf.random.normal(\n",
    "        shape, mean=self.mean, stddev=self.std, dtype=dtype), tf.int32), tf.float32)\n",
    "\n",
    "  def get_config(self):  # To support serialization\n",
    "    return {\"mean\": self.mean, \"std\": self.std}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d70a765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define forward propagation\n",
    "def forward_propagation(x, w_1, b_1, w_2, b_2):\n",
    "    h = np.matmul(x, w_1) + b_1\n",
    "    h_relu = np.where(h < 0, 0, h)\n",
    "    y = np.matmul(h_relu, w_2) + b_2\n",
    "    \n",
    "    return {'hidden': h, 'hidden_relu': h_relu, 'prediction': y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcf83c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training data using the true weights\n",
    "np.random.seed(seed=73)\n",
    "x_1 = [-4] + [int(x) for x in np.random.uniform(-2, 4, 100)]\n",
    "x_2 = [1] + [int(x) for x in np.random.uniform(-2, 4, 100)]\n",
    "x = np.transpose(np.array([x_1, x_2]))\n",
    "\n",
    "y = forward_propagation(x, w_1, b_1, w_2, b_2)['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7e4ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert training data to tf.data.Dataset with batch size\n",
    "batch_size = 1\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286a54f",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9328b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple NN with the pre-defined intialization:\n",
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "\n",
    "x1 = tf.keras.layers.Dense(3,\n",
    "                           activation=\"relu\",\n",
    "                           name=\"hidden\",\n",
    "                           kernel_initializer=MyInit(1, 1),\n",
    "                           bias_initializer=MyInit(2, 1))(inputs)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(1,\n",
    "                                name=\"predictions\",\n",
    "                                kernel_initializer=MyInit(1, 3),\n",
    "                                bias_initializer=MyInit(2, 0))(x1)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# set the loss and the optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9cff8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTS OF THE MODEL \n",
      "\n",
      "Hidden layer weights: \n",
      "[[0. 1. 0.]\n",
      " [2. 1. 1.]] \n",
      "\n",
      "Hidden layer bias: \n",
      "[0. 2. 1.] \n",
      "\n",
      "Output layer weights: \n",
      "[[-3.]\n",
      " [ 3.]\n",
      " [ 0.]] \n",
      "\n",
      "Hidden layer bias: \n",
      "[2.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Look at the initialized weights\n",
    "w1 = model.layers[1].weights[0].numpy()\n",
    "b1 = model.layers[1].weights[1].numpy()\n",
    "\n",
    "w2 = model.layers[2].weights[0].numpy()\n",
    "b2 = model.layers[2].weights[1].numpy()\n",
    "\n",
    "# Print the ouput nicely\n",
    "print(\"WEIGHTS OF THE MODEL \\n\")\n",
    "print(\"Hidden layer weights: \\n{} \\n\".format(w1))\n",
    "print(\"Hidden layer bias: \\n{} \\n\".format(b1))\n",
    "print(\"Output layer weights: \\n{} \\n\".format(w2))\n",
    "print(\"Hidden layer bias: \\n{} \\n\".format(b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5facadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPAGATED VALUES \n",
      "\n",
      "Input vector \n",
      "[-4  1] \n",
      "\n",
      "Hidden layer before activation \n",
      "[[ 2. -1.  2.]] \n",
      "\n",
      "Hidden layer after relu \n",
      "[[2. 0. 2.]] \n",
      "\n",
      "Prediction of output layer \n",
      "[[-4.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the forward propagation\n",
    "fp = forward_propagation(x[0:1], w1, b1, w2, b2)\n",
    "\n",
    "# Print the ouput nicely\n",
    "print(\"PROPAGATED VALUES \\n\")\n",
    "print(\"Input vector \\n{} \\n\".format(x[0]))\n",
    "print(\"Hidden layer before activation \\n{} \\n\".format(fp[\"hidden\"]))\n",
    "print(\"Hidden layer after relu \\n{} \\n\".format(fp[\"hidden_relu\"]))\n",
    "print(\"Prediction of output layer \\n{} \\n\".format(fp[\"prediction\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e10b3f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-4,  1]]), array([[1]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the first batch\n",
    "for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "    break\n",
    "\n",
    "x_batch_train.numpy(), y_batch_train.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe005d03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:[[-4.]], Target:[1], Loss:[25.]\n",
      "\n",
      ">>> GRADIENTS:\n",
      "Hidden layer weights \n",
      "[[-120.    0.    0.]\n",
      " [  30.   -0.   -0.]] \n",
      "\n",
      "Hidden layer bias \n",
      "[30. -0. -0.] \n",
      "\n",
      "Output layer weights \n",
      "[[-20.]\n",
      " [ -0.]\n",
      " [-20.]] \n",
      "\n",
      "Output layer bias \n",
      "[-10.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the gradients on the batch:\n",
    "with tf.GradientTape() as tape:\n",
    "    prediction = model(x_batch_train, training=True)\n",
    "    loss_value = loss_fn(y_batch_train, prediction)\n",
    "\n",
    "\n",
    "grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "# Print the ouput nicely\n",
    "print(\"Prediction:{}, Target:{}, Loss:{}\".format(prediction, y[0], loss_value))\n",
    "print(\"\\n>>> GRADIENTS:\")\n",
    "print(\"Hidden layer weights \\n{} \\n\".format(grads[0].numpy()))\n",
    "print(\"Hidden layer bias \\n{} \\n\".format(grads[1].numpy()))\n",
    "print(\"Output layer weights \\n{} \\n\".format(grads[2].numpy()))\n",
    "print(\"Output layer bias \\n{} \\n\".format(grads[3].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b94821ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply one SGD update\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e78902f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATED WEIGHTS OF THE MODEL \n",
      "\n",
      "Hidden layer weights: \n",
      "[[1.1999999 1.        0.       ]\n",
      " [1.7       1.        1.       ]] \n",
      "\n",
      "Hidden layer bias: \n",
      "[-0.29999998  2.          1.        ] \n",
      "\n",
      "Output layer weights: \n",
      "[[-2.8       ]\n",
      " [ 3.        ]\n",
      " [ 0.19999999]] \n",
      "\n",
      "Hidden layer bias: \n",
      "[2.1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the weights after the update:\n",
    "w1 = model.layers[1].weights[0].numpy()\n",
    "b1 = model.layers[1].weights[1].numpy()\n",
    "\n",
    "w2 = model.layers[2].weights[0].numpy()\n",
    "b2 = model.layers[2].weights[1].numpy()\n",
    "\n",
    "# Print the ouput nicely\n",
    "print(\"UPDATED WEIGHTS OF THE MODEL \\n\")\n",
    "print(\"Hidden layer weights: \\n{} \\n\".format(w1))\n",
    "print(\"Hidden layer bias: \\n{} \\n\".format(b1))\n",
    "print(\"Output layer weights: \\n{} \\n\".format(w2))\n",
    "print(\"Hidden layer bias: \\n{} \\n\".format(b2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72ec049d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:[[2.5]], Target:[1], Loss:[2.25]\n"
     ]
    }
   ],
   "source": [
    "# check the prediction\n",
    "prediction = model(x_batch_train, training=True)\n",
    "loss_value = loss_fn(y_batch_train, prediction)\n",
    "\n",
    "print(\"Prediction:{}, Target:{}, Loss:{}\".format(prediction, y[0], loss_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ab5fd",
   "metadata": {},
   "source": [
    "### Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90b51dd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      ">>> Loss 0.720\n",
      "\n",
      "Start of epoch 1\n",
      ">>> Loss 0.361\n",
      "\n",
      "Start of epoch 2\n",
      ">>> Loss 0.311\n",
      "\n",
      "Start of epoch 3\n",
      ">>> Loss 0.272\n",
      "\n",
      "Start of epoch 4\n",
      ">>> Loss 0.225\n",
      "\n",
      "Start of epoch 5\n",
      ">>> Loss 0.213\n",
      "\n",
      "Start of epoch 6\n",
      ">>> Loss 0.156\n",
      "\n",
      "Start of epoch 7\n",
      ">>> Loss 0.113\n",
      "\n",
      "Start of epoch 8\n",
      ">>> Loss 0.086\n",
      "\n",
      "Start of epoch 9\n",
      ">>> Loss 0.045\n",
      "\n",
      "Start of epoch 10\n",
      ">>> Loss 0.035\n",
      "\n",
      "Start of epoch 11\n",
      ">>> Loss 0.036\n",
      "\n",
      "Start of epoch 12\n",
      ">>> Loss 0.047\n",
      "\n",
      "Start of epoch 13\n",
      ">>> Loss 0.049\n",
      "\n",
      "Start of epoch 14\n",
      ">>> Loss 0.049\n",
      "\n",
      "Start of epoch 15\n",
      ">>> Loss 0.017\n"
     ]
    }
   ],
   "source": [
    "epochs = 16\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            pred = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, pred)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        mse = np.mean((model(x).numpy() - y)**2)\n",
    "        \n",
    "    print('>>> Loss {:.3f}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd7d5fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer weights: \n",
      "[[ 0.8380496   1.8128631  -0.25347403]\n",
      " [-0.95671076  0.66604346  0.6694228 ]] \n",
      "\n",
      "Hidden layer bias: \n",
      "[1.8954308  0.5246789  0.66958815] \n",
      "\n",
      "Output layer weights: \n",
      "[[-1.9003438 ]\n",
      " [ 1.4643725 ]\n",
      " [ 0.35568428]] \n",
      "\n",
      "Hidden layer bias: \n",
      "[0.5498195] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1 = model.layers[1].weights[0].numpy()\n",
    "b1 = model.layers[1].weights[1].numpy()\n",
    "\n",
    "w2 = model.layers[2].weights[0].numpy()\n",
    "b2 = model.layers[2].weights[1].numpy()\n",
    "\n",
    "print(\"Hidden layer weights: \\n{} \\n\".format(w1))\n",
    "print(\"Hidden layer bias: \\n{} \\n\".format(b1))\n",
    "print(\"Output layer weights: \\n{} \\n\".format(w2))\n",
    "print(\"Hidden layer bias: \\n{} \\n\".format(b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a87f827f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01748505669685138"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((model(x).numpy() - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ecf166",
   "metadata": {},
   "source": [
    "### Playing with optimizer and learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf849e00",
   "metadata": {},
   "source": [
    "http://2.bp.blogspot.com/-q6l20Vs4P_w/VPmIC7sEhnI/AAAAAAAACC4/g3UOUX2r_yA/s400/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b120d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "537c0461",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "\n",
    "x1 = tf.keras.layers.Dense(3,\n",
    "                           activation=\"relu\",\n",
    "                           name=\"hidden\",\n",
    "                           kernel_initializer=MyInit(1, 1),\n",
    "                           bias_initializer=MyInit(2, 1))(inputs)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(1,\n",
    "                                name=\"predictions\",\n",
    "                                kernel_initializer=MyInit(1, 3),\n",
    "                                bias_initializer=MyInit(2, 0))(x1)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ef3376f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      ">>> Loss 10.0086\n",
      "\n",
      "Start of epoch 1\n",
      ">>> Loss 6.3836\n",
      "\n",
      "Start of epoch 2\n",
      ">>> Loss 4.1899\n",
      "\n",
      "Start of epoch 3\n",
      ">>> Loss 2.4007\n",
      "\n",
      "Start of epoch 4\n",
      ">>> Loss 1.8415\n",
      "\n",
      "Start of epoch 5\n",
      ">>> Loss 1.3954\n",
      "\n",
      "Start of epoch 6\n",
      ">>> Loss 1.1239\n",
      "\n",
      "Start of epoch 7\n",
      ">>> Loss 0.9543\n",
      "\n",
      "Start of epoch 8\n",
      ">>> Loss 0.8465\n",
      "\n",
      "Start of epoch 9\n",
      ">>> Loss 0.7631\n",
      "\n",
      "Start of epoch 10\n",
      ">>> Loss 0.6984\n",
      "\n",
      "Start of epoch 11\n",
      ">>> Loss 0.6488\n",
      "\n",
      "Start of epoch 12\n",
      ">>> Loss 0.6150\n",
      "\n",
      "Start of epoch 13\n",
      ">>> Loss 0.5858\n",
      "\n",
      "Start of epoch 14\n",
      ">>> Loss 0.5604\n",
      "\n",
      "Start of epoch 15\n",
      ">>> Loss 0.5381\n",
      "\n",
      "Start of epoch 16\n",
      ">>> Loss 0.5183\n",
      "\n",
      "Start of epoch 17\n",
      ">>> Loss 0.5006\n",
      "\n",
      "Start of epoch 18\n",
      ">>> Loss 0.4847\n",
      "\n",
      "Start of epoch 19\n",
      ">>> Loss 0.4703\n",
      "\n",
      "Start of epoch 20\n",
      ">>> Loss 0.4572\n",
      "\n",
      "Start of epoch 21\n",
      ">>> Loss 0.4452\n",
      "\n",
      "Start of epoch 22\n",
      ">>> Loss 0.4343\n",
      "\n",
      "Start of epoch 23\n",
      ">>> Loss 0.4242\n",
      "\n",
      "Start of epoch 24\n",
      ">>> Loss 0.4150\n",
      "\n",
      "Start of epoch 25\n",
      ">>> Loss 0.4065\n",
      "\n",
      "Start of epoch 26\n",
      ">>> Loss 0.3987\n",
      "\n",
      "Start of epoch 27\n",
      ">>> Loss 0.3915\n",
      "\n",
      "Start of epoch 28\n",
      ">>> Loss 0.3849\n",
      "\n",
      "Start of epoch 29\n",
      ">>> Loss 0.3787\n",
      "\n",
      "Start of epoch 30\n",
      ">>> Loss 0.3730\n",
      "\n",
      "Start of epoch 31\n",
      ">>> Loss 0.3677\n",
      "\n",
      "Start of epoch 32\n",
      ">>> Loss 0.3629\n",
      "\n",
      "Start of epoch 33\n",
      ">>> Loss 0.3275\n",
      "\n",
      "Start of epoch 34\n",
      ">>> Loss 0.2865\n",
      "\n",
      "Start of epoch 35\n",
      ">>> Loss 0.2589\n",
      "\n",
      "Start of epoch 36\n",
      ">>> Loss 0.2349\n",
      "\n",
      "Start of epoch 37\n",
      ">>> Loss 0.2192\n",
      "\n",
      "Start of epoch 38\n",
      ">>> Loss 0.2088\n",
      "\n",
      "Start of epoch 39\n",
      ">>> Loss 0.2018\n",
      "\n",
      "Start of epoch 40\n",
      ">>> Loss 0.1188\n",
      "\n",
      "Start of epoch 41\n",
      ">>> Loss 0.0706\n",
      "\n",
      "Start of epoch 42\n",
      ">>> Loss 0.0523\n",
      "\n",
      "Start of epoch 43\n",
      ">>> Loss 0.0488\n",
      "\n",
      "Start of epoch 44\n",
      ">>> Loss 0.0465\n",
      "\n",
      "Start of epoch 45\n",
      ">>> Loss 0.0446\n",
      "\n",
      "Start of epoch 46\n",
      ">>> Loss 0.0428\n",
      "\n",
      "Start of epoch 47\n",
      ">>> Loss 0.0413\n",
      "\n",
      "Start of epoch 48\n",
      ">>> Loss 0.0398\n",
      "\n",
      "Start of epoch 49\n",
      ">>> Loss 0.0384\n",
      "\n",
      "Start of epoch 50\n",
      ">>> Loss 0.0372\n",
      "\n",
      "Start of epoch 51\n",
      ">>> Loss 0.0360\n",
      "\n",
      "Start of epoch 52\n",
      ">>> Loss 0.0348\n",
      "\n",
      "Start of epoch 53\n",
      ">>> Loss 0.0338\n",
      "\n",
      "Start of epoch 54\n",
      ">>> Loss 0.0327\n",
      "\n",
      "Start of epoch 55\n",
      ">>> Loss 0.0319\n",
      "\n",
      "Start of epoch 56\n",
      ">>> Loss 0.0309\n",
      "\n",
      "Start of epoch 57\n",
      ">>> Loss 0.0301\n",
      "\n",
      "Start of epoch 58\n",
      ">>> Loss 0.0294\n",
      "\n",
      "Start of epoch 59\n",
      ">>> Loss 0.0286\n",
      "\n",
      "Start of epoch 60\n",
      ">>> Loss 0.0281\n",
      "\n",
      "Start of epoch 61\n",
      ">>> Loss 0.0274\n",
      "\n",
      "Start of epoch 62\n",
      ">>> Loss 0.0268\n",
      "\n",
      "Start of epoch 63\n",
      ">>> Loss 0.0263\n",
      "\n",
      "Start of epoch 64\n",
      ">>> Loss 0.0258\n",
      "\n",
      "Start of epoch 65\n",
      ">>> Loss 0.0253\n",
      "\n",
      "Start of epoch 66\n",
      ">>> Loss 0.0249\n",
      "\n",
      "Start of epoch 67\n",
      ">>> Loss 0.0245\n",
      "\n",
      "Start of epoch 68\n",
      ">>> Loss 0.0241\n",
      "\n",
      "Start of epoch 69\n",
      ">>> Loss 0.0237\n",
      "\n",
      "Start of epoch 70\n",
      ">>> Loss 0.0234\n",
      "\n",
      "Start of epoch 71\n",
      ">>> Loss 0.0230\n",
      "\n",
      "Start of epoch 72\n",
      ">>> Loss 0.0227\n",
      "\n",
      "Start of epoch 73\n",
      ">>> Loss 0.0224\n",
      "\n",
      "Start of epoch 74\n",
      ">>> Loss 0.0221\n",
      "\n",
      "Start of epoch 75\n",
      ">>> Loss 0.0218\n",
      "\n",
      "Start of epoch 76\n",
      ">>> Loss 0.0215\n",
      "\n",
      "Start of epoch 77\n",
      ">>> Loss 0.0212\n",
      "\n",
      "Start of epoch 78\n",
      ">>> Loss 0.0209\n",
      "\n",
      "Start of epoch 79\n",
      ">>> Loss 0.0207\n",
      "\n",
      "Start of epoch 80\n",
      ">>> Loss 0.0205\n",
      "\n",
      "Start of epoch 81\n",
      ">>> Loss 0.0203\n",
      "\n",
      "Start of epoch 82\n",
      ">>> Loss 0.0201\n",
      "\n",
      "Start of epoch 83\n",
      ">>> Loss 0.0199\n",
      "\n",
      "Start of epoch 84\n",
      ">>> Loss 0.0197\n",
      "\n",
      "Start of epoch 85\n",
      ">>> Loss 0.0195\n",
      "\n",
      "Start of epoch 86\n",
      ">>> Loss 0.0193\n",
      "\n",
      "Start of epoch 87\n",
      ">>> Loss 0.0192\n",
      "\n",
      "Start of epoch 88\n",
      ">>> Loss 0.0190\n",
      "\n",
      "Start of epoch 89\n",
      ">>> Loss 0.0188\n",
      "\n",
      "Start of epoch 90\n",
      ">>> Loss 0.0187\n",
      "\n",
      "Start of epoch 91\n",
      ">>> Loss 0.0185\n",
      "\n",
      "Start of epoch 92\n",
      ">>> Loss 0.0184\n",
      "\n",
      "Start of epoch 93\n",
      ">>> Loss 0.0182\n",
      "\n",
      "Start of epoch 94\n",
      ">>> Loss 0.0181\n",
      "\n",
      "Start of epoch 95\n",
      ">>> Loss 0.0180\n",
      "\n",
      "Start of epoch 96\n",
      ">>> Loss 0.0178\n",
      "\n",
      "Start of epoch 97\n",
      ">>> Loss 0.0177\n",
      "\n",
      "Start of epoch 98\n",
      ">>> Loss 0.0176\n",
      "\n",
      "Start of epoch 99\n",
      ">>> Loss 0.0175\n",
      "\n",
      "Start of epoch 100\n",
      ">>> Loss 0.0173\n",
      "\n",
      "Start of epoch 101\n",
      ">>> Loss 0.0172\n",
      "\n",
      "Start of epoch 102\n",
      ">>> Loss 0.0171\n",
      "\n",
      "Start of epoch 103\n",
      ">>> Loss 0.0170\n",
      "\n",
      "Start of epoch 104\n",
      ">>> Loss 0.0169\n",
      "\n",
      "Start of epoch 105\n",
      ">>> Loss 0.0168\n",
      "\n",
      "Start of epoch 106\n",
      ">>> Loss 0.0167\n",
      "\n",
      "Start of epoch 107\n",
      ">>> Loss 0.0166\n",
      "\n",
      "Start of epoch 108\n",
      ">>> Loss 0.0165\n",
      "\n",
      "Start of epoch 109\n",
      ">>> Loss 0.0164\n",
      "\n",
      "Start of epoch 110\n",
      ">>> Loss 0.0163\n",
      "\n",
      "Start of epoch 111\n",
      ">>> Loss 0.0162\n",
      "\n",
      "Start of epoch 112\n",
      ">>> Loss 0.0162\n",
      "\n",
      "Start of epoch 113\n",
      ">>> Loss 0.0161\n",
      "\n",
      "Start of epoch 114\n",
      ">>> Loss 0.0160\n",
      "\n",
      "Start of epoch 115\n",
      ">>> Loss 0.0159\n",
      "\n",
      "Start of epoch 116\n",
      ">>> Loss 0.0158\n",
      "\n",
      "Start of epoch 117\n",
      ">>> Loss 0.0158\n",
      "\n",
      "Start of epoch 118\n",
      ">>> Loss 0.0157\n",
      "\n",
      "Start of epoch 119\n",
      ">>> Loss 0.0156\n",
      "\n",
      "Start of epoch 120\n",
      ">>> Loss 0.0156\n",
      "\n",
      "Start of epoch 121\n",
      ">>> Loss 0.0155\n",
      "\n",
      "Start of epoch 122\n",
      ">>> Loss 0.0154\n",
      "\n",
      "Start of epoch 123\n",
      ">>> Loss 0.0154\n",
      "\n",
      "Start of epoch 124\n",
      ">>> Loss 0.0153\n",
      "\n",
      "Start of epoch 125\n",
      ">>> Loss 0.0152\n",
      "\n",
      "Start of epoch 126\n",
      ">>> Loss 0.0152\n",
      "\n",
      "Start of epoch 127\n",
      ">>> Loss 0.0151\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "epochs = 128\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            pred = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, pred)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        mse = np.mean((model(x).numpy() - y)**2)\n",
    "        \n",
    "    print('>>> Loss {:.4f}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f1840b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer weights: \n",
      "[[ 0.68046284  1.3424093   0.07904674]\n",
      " [-0.70974535  0.487307    0.7084906 ]] \n",
      "\n",
      "Hidden layer bias: \n",
      "[1.459933   0.46041295 1.1783967 ] \n",
      "\n",
      "Output layer weights: \n",
      "[[-2.6617184 ]\n",
      " [ 2.0708191 ]\n",
      " [ 0.14849895]] \n",
      "\n",
      "Hidden layer bias: \n",
      "[0.79769206] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1 = model.layers[1].weights[0].numpy()\n",
    "b1 = model.layers[1].weights[1].numpy()\n",
    "\n",
    "w2 = model.layers[2].weights[0].numpy()\n",
    "b2 = model.layers[2].weights[1].numpy()\n",
    "\n",
    "print(\"Hidden layer weights: \\n{} \\n\".format(w1))\n",
    "print(\"Hidden layer bias: \\n{} \\n\".format(b1))\n",
    "print(\"Output layer weights: \\n{} \\n\".format(w2))\n",
    "print(\"Hidden layer bias: \\n{} \\n\".format(b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc2dd3",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**TO DO:** Try different optimizers with different lerning rate, how does the convergence look? Does it converge? How fast? Which one is the best?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21448740",
   "metadata": {},
   "source": [
    "## Vanishing gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5e4dc66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_values (InputLayer)   [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 9         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 12        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 12        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 12        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 12        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 12        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 12        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 3)                 12        \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 3)                 12        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109\n",
      "Trainable params: 109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create a simple NN with the pre-defined intialization:\n",
    "init = tf.keras.initializers.RandomUniform(minval=0, maxval=1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=2, name=\"input_values\")\n",
    "\n",
    "h = tf.keras.layers.Dense(3, activation=\"tanh\")(inputs)\n",
    "h = tf.keras.layers.Dense(3, activation=\"tanh\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(3, activation=\"tanh\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(3, activation=\"tanh\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(3, activation=\"tanh\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(3, activation=\"tanh\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(3, activation=\"tanh\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(3, activation=\"tanh\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(3, activation=\"tanh\", kernel_initializer=init)(h)\n",
    "outputs = tf.keras.layers.Dense(1)(h)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# set the loss and the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.MSE\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16bf262d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-4,  1],\n",
       "        [ 1,  0],\n",
       "        [ 1, -1],\n",
       "        [ 1,  2],\n",
       "        [ 1,  0],\n",
       "        [ 0, -1],\n",
       "        [ 0,  0],\n",
       "        [ 0,  0],\n",
       "        [ 1,  0],\n",
       "        [ 2,  1]]),\n",
       " array([[ 1],\n",
       "        [-1],\n",
       "        [-4],\n",
       "        [ 5],\n",
       "        [-1],\n",
       "        [-5],\n",
       "        [-2],\n",
       "        [-2],\n",
       "        [-1],\n",
       "        [ 3]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take the first batch\n",
    "for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "    break\n",
    "\n",
    "x_batch_train.numpy(), y_batch_train.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c606d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gradients on the batch:\n",
    "with tf.GradientTape() as tape:\n",
    "    prediction = model(x_batch_train, training=True)\n",
    "    loss_value = loss_fn(y_batch_train, prediction)\n",
    "\n",
    "grads = tape.gradient(loss_value, model.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db90cabf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:[[-0.47241157]\n",
      " [ 0.47160947]\n",
      " [ 0.45250666]\n",
      " [ 0.47169375]\n",
      " [ 0.47160947]\n",
      " [-0.47030455]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.47160947]\n",
      " [ 0.4719543 ]], Target:[1], Loss:[ 2.167996   2.1656344 19.824814  20.505556   2.1656344 20.51814\n",
      "  4.         4.         2.1656344  6.391015 ]\n",
      "\n",
      ">>> GRADIENTS:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 1 layer weights \n",
      "[[-0.59448403 -0.93147856 -0.8353819 ]\n",
      " [ 0.6401543   0.94288766  0.86660784]] \n",
      "\n",
      "# 1 layer bias \n",
      "[-22.293747 -31.642372 -30.468355] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 2 layer weights \n",
      "[[ 0.18893619  0.1239714   0.21618688]\n",
      " [-0.07571094 -0.03798299 -0.07319543]\n",
      " [ 0.10330427  0.08833829  0.14312653]] \n",
      "\n",
      "# 2 layer bias \n",
      "[-19.435312 -12.74421  -23.430132] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 3 layer weights \n",
      "[[0.12783748 0.04943584 0.09636923]\n",
      " [0.02881063 0.00904407 0.02525847]\n",
      " [0.08411351 0.0311726  0.06564118]] \n",
      "\n",
      "# 3 layer bias \n",
      "[-16.31608   -6.278863 -13.639667] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 4 layer weights \n",
      "[[0.04645368 0.0671518  0.02096234]\n",
      " [0.04450128 0.06436754 0.02154508]\n",
      " [0.07613736 0.11012484 0.04020236]] \n",
      "\n",
      "# 4 layer bias \n",
      "[ -7.246861  -11.164541   -4.3893814] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 5 layer weights \n",
      "[[0.11092923 0.03926804 0.07006904]\n",
      " [0.11058228 0.0388283  0.06932621]\n",
      " [0.02357379 0.01725624 0.02961873]] \n",
      "\n",
      "# 5 layer bias \n",
      "[-10.860298   -3.4858048  -5.960339 ] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 6 layer weights \n",
      "[[0.02739495 0.03319579 0.01248372]\n",
      " [0.08163505 0.12449268 0.02851576]\n",
      " [0.07916207 0.12059274 0.02769588]] \n",
      "\n",
      "# 6 layer bias \n",
      "[-5.067325 -8.409969 -2.518033] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 7 layer weights \n",
      "[[0.01614602 0.02790202 0.03194327]\n",
      " [0.01502781 0.02581077 0.02935282]\n",
      " [0.09536131 0.12630181 0.09703349]] \n",
      "\n",
      "# 7 layer bias \n",
      "[-4.9184017 -5.359677  -4.1178236] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 8 layer weights \n",
      "[[-0.00426183  0.03420763  0.00191498]\n",
      " [-0.00751119  0.01987738 -0.020407  ]\n",
      " [-0.00645384  0.03292905 -0.00820675]] \n",
      "\n",
      "# 8 layer bias \n",
      "[-0.42043746 -3.4972115  -3.172915  ] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 9 layer weights \n",
      "[[-2.0314437e-01 -3.5421388e-05 -1.6164286e-01]\n",
      " [-2.0658351e-01  3.0791874e-05 -1.6496107e-01]\n",
      " [-2.1981519e-01 -2.7011421e-05 -1.7500655e-01]] \n",
      "\n",
      "# 9 layer bias \n",
      "[-3.9508684  -0.01595019 -3.1395977 ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the ouput nicely\n",
    "print(\"Prediction:{}, Target:{}, Loss:{}\".format(prediction, y[0], loss_value))\n",
    "print(\"\\n>>> GRADIENTS:\")\n",
    "\n",
    "for i in range(0, 18, 2):\n",
    "    l = int(i / 2 + 1)\n",
    "    print('-' * 100)\n",
    "    print(\"# {} layer weights \\n{} \\n\".format(l, grads[i].numpy()))\n",
    "    print(\"# {} layer bias \\n{} \\n\".format(l, grads[i+1].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46d5086a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      ">>> Loss 12.566\n",
      "\n",
      "Start of epoch 1\n",
      ">>> Loss 12.381\n",
      "\n",
      "Start of epoch 2\n",
      ">>> Loss 12.107\n",
      "\n",
      "Start of epoch 3\n",
      ">>> Loss 11.637\n",
      "\n",
      "Start of epoch 4\n",
      ">>> Loss 11.428\n",
      "\n",
      "Start of epoch 5\n",
      ">>> Loss 11.274\n",
      "\n",
      "Start of epoch 6\n",
      ">>> Loss 11.134\n",
      "\n",
      "Start of epoch 7\n",
      ">>> Loss 11.004\n",
      "\n",
      "Start of epoch 8\n",
      ">>> Loss 10.879\n",
      "\n",
      "Start of epoch 9\n",
      ">>> Loss 10.761\n",
      "\n",
      "Start of epoch 10\n",
      ">>> Loss 10.648\n",
      "\n",
      "Start of epoch 11\n",
      ">>> Loss 10.540\n",
      "\n",
      "Start of epoch 12\n",
      ">>> Loss 10.436\n",
      "\n",
      "Start of epoch 13\n",
      ">>> Loss 10.336\n",
      "\n",
      "Start of epoch 14\n",
      ">>> Loss 10.240\n",
      "\n",
      "Start of epoch 15\n",
      ">>> Loss 10.148\n",
      "\n",
      "Start of epoch 16\n",
      ">>> Loss 10.059\n",
      "\n",
      "Start of epoch 17\n",
      ">>> Loss 9.973\n",
      "\n",
      "Start of epoch 18\n",
      ">>> Loss 9.891\n",
      "\n",
      "Start of epoch 19\n",
      ">>> Loss 9.811\n",
      "\n",
      "Start of epoch 20\n",
      ">>> Loss 9.735\n",
      "\n",
      "Start of epoch 21\n",
      ">>> Loss 9.661\n",
      "\n",
      "Start of epoch 22\n",
      ">>> Loss 9.590\n",
      "\n",
      "Start of epoch 23\n",
      ">>> Loss 9.521\n",
      "\n",
      "Start of epoch 24\n",
      ">>> Loss 9.455\n",
      "\n",
      "Start of epoch 25\n",
      ">>> Loss 9.392\n",
      "\n",
      "Start of epoch 26\n",
      ">>> Loss 9.331\n",
      "\n",
      "Start of epoch 27\n",
      ">>> Loss 9.272\n",
      "\n",
      "Start of epoch 28\n",
      ">>> Loss 9.215\n",
      "\n",
      "Start of epoch 29\n",
      ">>> Loss 9.160\n",
      "\n",
      "Start of epoch 30\n",
      ">>> Loss 9.108\n",
      "\n",
      "Start of epoch 31\n",
      ">>> Loss 9.057\n",
      "\n",
      "Start of epoch 32\n",
      ">>> Loss 9.008\n",
      "\n",
      "Start of epoch 33\n",
      ">>> Loss 8.961\n",
      "\n",
      "Start of epoch 34\n",
      ">>> Loss 8.916\n",
      "\n",
      "Start of epoch 35\n",
      ">>> Loss 8.873\n",
      "\n",
      "Start of epoch 36\n",
      ">>> Loss 8.831\n",
      "\n",
      "Start of epoch 37\n",
      ">>> Loss 8.791\n",
      "\n",
      "Start of epoch 38\n",
      ">>> Loss 8.753\n",
      "\n",
      "Start of epoch 39\n",
      ">>> Loss 8.716\n",
      "\n",
      "Start of epoch 40\n",
      ">>> Loss 8.681\n",
      "\n",
      "Start of epoch 41\n",
      ">>> Loss 8.647\n",
      "\n",
      "Start of epoch 42\n",
      ">>> Loss 8.614\n",
      "\n",
      "Start of epoch 43\n",
      ">>> Loss 8.583\n",
      "\n",
      "Start of epoch 44\n",
      ">>> Loss 8.553\n",
      "\n",
      "Start of epoch 45\n",
      ">>> Loss 8.524\n",
      "\n",
      "Start of epoch 46\n",
      ">>> Loss 8.497\n",
      "\n",
      "Start of epoch 47\n",
      ">>> Loss 8.470\n",
      "\n",
      "Start of epoch 48\n",
      ">>> Loss 8.445\n",
      "\n",
      "Start of epoch 49\n",
      ">>> Loss 8.421\n",
      "\n",
      "Start of epoch 50\n",
      ">>> Loss 8.398\n",
      "\n",
      "Start of epoch 51\n",
      ">>> Loss 8.376\n",
      "\n",
      "Start of epoch 52\n",
      ">>> Loss 8.354\n",
      "\n",
      "Start of epoch 53\n",
      ">>> Loss 8.334\n",
      "\n",
      "Start of epoch 54\n",
      ">>> Loss 8.315\n",
      "\n",
      "Start of epoch 55\n",
      ">>> Loss 8.296\n",
      "\n",
      "Start of epoch 56\n",
      ">>> Loss 8.279\n",
      "\n",
      "Start of epoch 57\n",
      ">>> Loss 8.262\n",
      "\n",
      "Start of epoch 58\n",
      ">>> Loss 8.246\n",
      "\n",
      "Start of epoch 59\n",
      ">>> Loss 8.231\n",
      "\n",
      "Start of epoch 60\n",
      ">>> Loss 8.216\n",
      "\n",
      "Start of epoch 61\n",
      ">>> Loss 8.202\n",
      "\n",
      "Start of epoch 62\n",
      ">>> Loss 8.189\n",
      "\n",
      "Start of epoch 63\n",
      ">>> Loss 8.176\n",
      "\n",
      "Start of epoch 64\n",
      ">>> Loss 8.164\n",
      "\n",
      "Start of epoch 65\n",
      ">>> Loss 8.153\n",
      "\n",
      "Start of epoch 66\n",
      ">>> Loss 8.142\n",
      "\n",
      "Start of epoch 67\n",
      ">>> Loss 8.132\n",
      "\n",
      "Start of epoch 68\n",
      ">>> Loss 8.122\n",
      "\n",
      "Start of epoch 69\n",
      ">>> Loss 8.113\n",
      "\n",
      "Start of epoch 70\n",
      ">>> Loss 8.104\n",
      "\n",
      "Start of epoch 71\n",
      ">>> Loss 8.096\n",
      "\n",
      "Start of epoch 72\n",
      ">>> Loss 8.088\n",
      "\n",
      "Start of epoch 73\n",
      ">>> Loss 8.080\n",
      "\n",
      "Start of epoch 74\n",
      ">>> Loss 8.073\n",
      "\n",
      "Start of epoch 75\n",
      ">>> Loss 8.066\n",
      "\n",
      "Start of epoch 76\n",
      ">>> Loss 8.060\n",
      "\n",
      "Start of epoch 77\n",
      ">>> Loss 8.053\n",
      "\n",
      "Start of epoch 78\n",
      ">>> Loss 8.048\n",
      "\n",
      "Start of epoch 79\n",
      ">>> Loss 8.042\n",
      "\n",
      "Start of epoch 80\n",
      ">>> Loss 8.037\n",
      "\n",
      "Start of epoch 81\n",
      ">>> Loss 8.032\n",
      "\n",
      "Start of epoch 82\n",
      ">>> Loss 8.028\n",
      "\n",
      "Start of epoch 83\n",
      ">>> Loss 8.023\n",
      "\n",
      "Start of epoch 84\n",
      ">>> Loss 8.019\n",
      "\n",
      "Start of epoch 85\n",
      ">>> Loss 8.015\n",
      "\n",
      "Start of epoch 86\n",
      ">>> Loss 8.012\n",
      "\n",
      "Start of epoch 87\n",
      ">>> Loss 8.008\n",
      "\n",
      "Start of epoch 88\n",
      ">>> Loss 8.005\n",
      "\n",
      "Start of epoch 89\n",
      ">>> Loss 8.002\n",
      "\n",
      "Start of epoch 90\n",
      ">>> Loss 7.999\n",
      "\n",
      "Start of epoch 91\n",
      ">>> Loss 7.996\n",
      "\n",
      "Start of epoch 92\n",
      ">>> Loss 7.993\n",
      "\n",
      "Start of epoch 93\n",
      ">>> Loss 7.991\n",
      "\n",
      "Start of epoch 94\n",
      ">>> Loss 7.989\n",
      "\n",
      "Start of epoch 95\n",
      ">>> Loss 7.987\n",
      "\n",
      "Start of epoch 96\n",
      ">>> Loss 7.985\n",
      "\n",
      "Start of epoch 97\n",
      ">>> Loss 7.983\n",
      "\n",
      "Start of epoch 98\n",
      ">>> Loss 7.981\n",
      "\n",
      "Start of epoch 99\n",
      ">>> Loss 7.979\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            pred = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, pred)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        mse = np.mean((model(x).numpy() - y)**2)\n",
    "        \n",
    "    print('>>> Loss {:.3f}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7f9787e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:[[-0.47241157]\n",
      " [ 0.47160947]\n",
      " [ 0.45250666]\n",
      " [ 0.47169375]\n",
      " [ 0.47160947]\n",
      " [-0.47030455]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.47160947]\n",
      " [ 0.4719543 ]], Target:[1], Loss:[1.9294467]\n",
      "\n",
      ">>> GRADIENTS:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 1 layer weights \n",
      "[[-1.8176455e-04 -8.3771029e-06 -1.2794870e-05]\n",
      " [-0.0000000e+00 -0.0000000e+00 -0.0000000e+00]] \n",
      "\n",
      "# 1 layer bias \n",
      "[-6.0588183e-05 -2.7923675e-06 -4.2649567e-06] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 2 layer weights \n",
      "[[-6.5348046e-05 -3.0091678e-05 -5.0795934e-05]\n",
      " [ 1.1727618e-04  5.4003714e-05  9.1160386e-05]\n",
      " [ 1.1654372e-04  5.3666427e-05  9.0591035e-05]] \n",
      "\n",
      "# 2 layer bias \n",
      "[-1.1837532e-04 -5.4509848e-05 -9.2014765e-05] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 3 layer weights \n",
      "[[9.0691057e-05 4.8606715e-05 4.2017586e-05]\n",
      " [1.4713530e-04 7.8858531e-05 6.8168461e-05]\n",
      " [1.3905157e-04 7.4525975e-05 6.4423235e-05]] \n",
      "\n",
      "# 3 layer bias \n",
      "[-1.7881885e-04 -9.5839619e-05 -8.2847597e-05] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 4 layer weights \n",
      "[[1.0978579e-04 1.6405460e-04 2.2239496e-04]\n",
      " [9.8321711e-05 1.4692364e-04 1.9917196e-04]\n",
      " [1.1920975e-04 1.7813696e-04 2.4148524e-04]] \n",
      "\n",
      "# 4 layer bias \n",
      "[-0.00013886 -0.0002075  -0.00028129] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 5 layer weights \n",
      "[[0.00134032 0.00013627 0.00025347]\n",
      " [0.00131486 0.00013368 0.00024866]\n",
      " [0.00103282 0.000105   0.00019532]] \n",
      "\n",
      "# 5 layer bias \n",
      "[-0.00147362 -0.00014982 -0.00027868] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 6 layer weights \n",
      "[[0.00078573 0.00155841 0.00011526]\n",
      " [0.00103482 0.00205246 0.0001518 ]\n",
      " [0.00101344 0.00201004 0.00014866]] \n",
      "\n",
      "# 6 layer bias \n",
      "[-0.00111967 -0.00222075 -0.00016424] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 7 layer weights \n",
      "[[0.00281467 0.00264558 0.0013837 ]\n",
      " [0.00269131 0.00252964 0.00132305]\n",
      " [0.00322551 0.00303175 0.00158567]] \n",
      "\n",
      "# 7 layer bias \n",
      "[-0.00332269 -0.00312309 -0.00163344] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 8 layer weights \n",
      "[[0.0033707  0.00229061 0.00634416]\n",
      " [0.00339626 0.00230798 0.00639226]\n",
      " [0.00351523 0.00238882 0.00661618]] \n",
      "\n",
      "# 8 layer bias \n",
      "[-0.0038817  -0.00263787 -0.00730594] \n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# 9 layer weights \n",
      "[[0.07655722 0.00255009 0.06595111]\n",
      " [0.07794189 0.00259622 0.06714395]\n",
      " [0.07653977 0.00254951 0.06593608]] \n",
      "\n",
      "# 9 layer bias \n",
      "[-0.07860417 -0.00261828 -0.06771448] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the gradients after some time of training\n",
    "print(\"Prediction:{}, Target:{}, Loss:{}\".format(prediction, y[0], loss_value))\n",
    "print(\"\\n>>> GRADIENTS:\")\n",
    "\n",
    "for i in range(0, 18, 2):\n",
    "    l = int(i / 2 + 1)\n",
    "    print('-' * 100)\n",
    "    print(\"# {} layer weights \\n{} \\n\".format(l, grads[i].numpy()))\n",
    "    print(\"# {} layer bias \\n{} \\n\".format(l, grads[i+1].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b544c1d",
   "metadata": {},
   "source": [
    "## Two circles example\n",
    "see https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ca57c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f145b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    samples_ix = np.where(y == i)\n",
    "    pyplot.scatter(X[samples_ix, 0], X[samples_ix, 1], label=str(i))\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d9494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model run for various model specifications\n",
    "def run(model, iterations):\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "    for r in range(iterations):\n",
    "        print(\"After {} episodes:\".format((r + 1)* 10))\n",
    "        history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=10, batch_size=100, verbose=0)\n",
    "        acc_train.extend(history.history['accuracy'])\n",
    "        acc_test.extend(history.history['val_accuracy'])\n",
    "\n",
    "        prediction = model.predict(trainX)\n",
    "        prediction = np.where(model.predict(trainX) > 0.5, 1, 0)\n",
    "\n",
    "        for i in range(2):\n",
    "            samples_ix = np.where(prediction == i)\n",
    "            pyplot.scatter(X[samples_ix, 0], X[samples_ix, 1], label=str(i))\n",
    "        pyplot.legend()\n",
    "        pyplot.show()\n",
    "\n",
    "    pyplot.plot(acc_train, label='train')\n",
    "    pyplot.plot(acc_test, label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "    \n",
    "    print('Evaluation')\n",
    "    model.evaluate(testX, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d988648",
   "metadata": {},
   "source": [
    "### Models with ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847945b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "init = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "h = tf.keras.layers.Dense(2, activation=\"relu\", kernel_initializer=init)(inputs)\n",
    "outputs = tf.keras.layers.Dense(1)(h)\n",
    "outputs = tf.keras.activations.sigmoid(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "run(model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc05f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "acc_train = []\n",
    "acc_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b040bc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "init = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "h = tf.keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)(inputs)\n",
    "outputs = tf.keras.layers.Dense(1)(h)\n",
    "outputs = tf.keras.activations.sigmoid(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "run(model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065df85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "init = tf.keras.initializers.RandomUniform(minval=0, maxval=1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "h = tf.keras.layers.Dense(5, activation=\"relu\", kernel_initializer=init)(inputs)\n",
    "h = tf.keras.layers.Dense(5, activation=\"relu\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(5, activation=\"relu\", kernel_initializer=init)(h)\n",
    "outputs = tf.keras.layers.Dense(1)(h)\n",
    "outputs = tf.keras.activations.sigmoid(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "run(model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcff372",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "init = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "h = tf.keras.layers.Dense(5, activation=\"relu\", kernel_initializer=init)(inputs)\n",
    "h = tf.keras.layers.Dense(5, activation=\"relu\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(5, activation=\"relu\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(5, activation=\"relu\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(5, activation=\"relu\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(5, activation=\"relu\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(5, activation=\"relu\", kernel_initializer=init)(h)\n",
    "outputs = tf.keras.layers.Dense(1)(h)\n",
    "outputs = tf.keras.activations.sigmoid(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "run(model, 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec06ff",
   "metadata": {},
   "source": [
    "### Models with tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f4b63b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "init = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "h = tf.keras.layers.Dense(2, activation=\"tanh\", kernel_initializer=init)(inputs)\n",
    "outputs = tf.keras.layers.Dense(1)(h)\n",
    "outputs = tf.keras.activations.sigmoid(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "run(model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe3b621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "init = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "h = tf.keras.layers.Dense(10, activation=\"tanh\", kernel_initializer=init)(inputs)\n",
    "outputs = tf.keras.layers.Dense(1)(h)\n",
    "outputs = tf.keras.activations.sigmoid(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "run(model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34c213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "init = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "h = tf.keras.layers.Dense(5, activation=\"tanh\", kernel_initializer=init)(inputs)\n",
    "h = tf.keras.layers.Dense(5, activation=\"tanh\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(5, activation=\"tanh\", kernel_initializer=init)(h)\n",
    "outputs = tf.keras.layers.Dense(1)(h)\n",
    "outputs = tf.keras.activations.sigmoid(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "run(model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd4426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "init = tf.keras.initializers.RandomUniform(minval=0, maxval=1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "h = tf.keras.layers.Dense(5, activation=\"tanh\", kernel_initializer=init)(inputs)\n",
    "h = tf.keras.layers.Dense(5, activation=\"tanh\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(5, activation=\"tanh\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(5, activation=\"tanh\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(5, activation=\"tanh\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(5, activation=\"tanh\", kernel_initializer=init)(h)\n",
    "outputs = tf.keras.layers.Dense(1)(h)\n",
    "outputs = tf.keras.activations.sigmoid(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "run(model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1dc32d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa7476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the gradients\n",
    "with tf.GradientTape() as tape:\n",
    "    prediction = model(trainX, training=True)\n",
    "    loss_value = loss_fn(testy, prediction)\n",
    "\n",
    "grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "print(\"\\n>>> GRADIENTS:\")\n",
    "\n",
    "for i in range(len(model.layers)):\n",
    "    l = int((i + 1) / 2)\n",
    "    print('-' * 100)\n",
    "    print(\"# {} layer weights \\n{} \\n\".format(l, grads[i].numpy()))\n",
    "    print(\"# {} layer bias \\n{} \\n\".format(l, grads[i+1].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11fdeb",
   "metadata": {},
   "source": [
    "### Models without activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef945286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init = tf.keras.initializers.RandomUniform(minval=0, maxval=1)\n",
    "init = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "h = tf.keras.layers.Dense(5, activation=None, kernel_initializer=init)(inputs)\n",
    "h = tf.keras.layers.Dense(5, activation=None, kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(5, activation=None, kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(5, activation=None, kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(5, activation=None, kernel_initializer=init)(h)\n",
    "outputs = tf.keras.layers.Dense(1)(h)\n",
    "outputs = tf.keras.activations.sigmoid(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "run(model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb490f9b",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**TO DO:** Similar to the accuracy, store and plot also the train and test loss. Inspect the loss evolution.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b43e8d",
   "metadata": {},
   "source": [
    "### Multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb087dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e1bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, \n",
    "                           n_redundant=0, n_classes=4, random_state=1, n_clusters_per_class=1,\n",
    "                           class_sep=2)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "# split into train and test\n",
    "n_train = 500\n",
    "\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "for i in range(4):\n",
    "    samples_ix = np.where(y == i)\n",
    "    pyplot.scatter(X[samples_ix, 0], X[samples_ix, 1], label=str(i))\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a23d753",
   "metadata": {},
   "source": [
    "#### ReLU models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a7ef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define model run for various model specifications\n",
    "def run(model):\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    model.compile(loss=loss_fn, optimizer=optimizer, metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "    for r in range(20):\n",
    "        print(\"After {} episodes:\".format((r + 1)* 10))\n",
    "        history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=10, verbose=0)\n",
    "        acc_train.extend(history.history['sparse_categorical_accuracy'])\n",
    "        acc_test.extend(history.history['val_sparse_categorical_accuracy'])\n",
    "\n",
    "        prediction = model.predict(trainX)\n",
    "        prediction = np.argmax(model.predict(trainX), axis=1)\n",
    "\n",
    "        for i in range(4):\n",
    "            samples_ix = np.where(prediction == i)\n",
    "            pyplot.scatter(X[samples_ix, 0], X[samples_ix, 1], label=str(i))\n",
    "        pyplot.legend()\n",
    "        pyplot.show()\n",
    "\n",
    "    pyplot.plot(acc_train, label='train')\n",
    "    pyplot.plot(acc_test, label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "\n",
    "    print('Evaluation')\n",
    "    model.evaluate(testX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710a0aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "init = tf.keras.initializers.RandomUniform(minval=0, maxval=1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "h = tf.keras.layers.Dense(5, activation=\"relu\", kernel_initializer=init)(inputs)\n",
    "outputs = tf.keras.layers.Dense(4)(h)\n",
    "outputs = tf.keras.activations.softmax(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf150ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "init = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "h = tf.keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)(inputs)\n",
    "h = tf.keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)(h)\n",
    "outputs = tf.keras.layers.Dense(4)(h)\n",
    "outputs = tf.keras.activations.softmax(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13be908",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "init = tf.keras.initializers.RandomUniform(minval=0, maxval=1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "h = tf.keras.layers.Dense(5, activation=\"relu\", kernel_initializer=init)(inputs)\n",
    "h = tf.keras.layers.Dense(5, activation=\"relu\", kernel_initializer=init)(h)\n",
    "h = tf.keras.layers.Dense(5, activation=\"relu\", kernel_initializer=init)(h)\n",
    "outputs = tf.keras.layers.Dense(4)(h)\n",
    "outputs = tf.keras.activations.softmax(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb669198",
   "metadata": {},
   "source": [
    "### Complex multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66727bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016eee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model run for various model specifications\n",
    "def run(model):\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    model.compile(loss=loss_fn, optimizer=optimizer, metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "    for r in range(20):\n",
    "        print(\"After {} episodes:\".format((r + 1)* 10))\n",
    "        history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=10, verbose=0)\n",
    "        acc_train.extend(history.history['sparse_categorical_accuracy'])\n",
    "        acc_test.extend(history.history['val_sparse_categorical_accuracy'])\n",
    "\n",
    "        prediction = model.predict(trainX)\n",
    "        prediction = np.argmax(model.predict(trainX), axis=1)\n",
    "\n",
    "        for i in range(4):\n",
    "            samples_ix = np.where(prediction == i)\n",
    "            pyplot.scatter(trainX[samples_ix, 0], trainX[samples_ix, 1], label=str(i))\n",
    "        pyplot.legend()\n",
    "        pyplot.show()\n",
    "\n",
    "    pyplot.plot(acc_train, label='train')\n",
    "    pyplot.plot(acc_test, label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "\n",
    "    print('Evaluation')\n",
    "    model.evaluate(testX, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc612a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "X_1, y_1 = make_moons(n_samples=1000, random_state=42, noise=0.2)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_1 = scaler.fit_transform(X_1)\n",
    "\n",
    "X_2, y_2 = make_moons(n_samples=1000, random_state=42, noise=0.2)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_2 = scaler.fit_transform(X_1)\n",
    "\n",
    "X = np.append(X_1, X_2 * np.array([1.2, 0.8]) + np.array([-1, -1]), axis=0)\n",
    "y = np.append(y_1, y_2 + 2, axis=0)\n",
    "\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, random_state=3)\n",
    "\n",
    "for i in range(4):\n",
    "    samples_ix = np.where(y == i)\n",
    "    pyplot.scatter(X[samples_ix, 0], X[samples_ix, 1], label=str(i))\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea73ba",
   "metadata": {},
   "source": [
    "#### Model with ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f177d8ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "init = tf.keras.initializers.RandomUniform(minval=0, maxval=1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "h = tf.keras.layers.Dense(5, activation=\"relu\", kernel_initializer=init)(inputs)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(4)(h)\n",
    "outputs = tf.keras.activations.softmax(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511bd871",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "init = tf.keras.initializers.RandomUniform(minval=0, maxval=1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "h = tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=init)(inputs)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(4)(h)\n",
    "outputs = tf.keras.activations.softmax(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf2bd51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "init = tf.keras.initializers.RandomUniform(minval=0, maxval=1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(2,), name=\"input_values\")\n",
    "h = tf.keras.layers.Dense(50, activation=\"relu\", kernel_initializer=init)(inputs)\n",
    "h = tf.keras.layers.Dense(50, activation=\"relu\", kernel_initializer=init)(inputs)\n",
    "h = tf.keras.layers.Dense(50, activation=\"relu\", kernel_initializer=init)(inputs)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(4)(h)\n",
    "outputs = tf.keras.activations.softmax(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run(model, iterations):\n",
    "#     optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "#     loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "#     model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "#     acc_train = []\n",
    "#     acc_test = []\n",
    "#     loss_train = []\n",
    "#     loss_test = []\n",
    "\n",
    "#     for r in range(iterations):\n",
    "#         print(\"After {} episodes:\".format((r + 1) * 10))\n",
    "#         history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=10, batch_size=100, verbose=0)\n",
    "#         acc_train.extend(history.history['accuracy'])\n",
    "#         acc_test.extend(history.history['val_accuracy'])\n",
    "#         loss_train.extend(history.history['loss'])\n",
    "#         loss_test.extend(history.history['val_loss'])\n",
    "\n",
    "#         prediction = model.predict(trainX)\n",
    "#         prediction = np.where(model.predict(trainX) > 0.5, 1, 0)\n",
    "\n",
    "#         for i in range(2):\n",
    "#             samples_ix = np.where(prediction == i)\n",
    "#             pyplot.scatter(X[samples_ix, 0], X[samples_ix, 1], label=str(i))\n",
    "#         pyplot.legend()\n",
    "#         pyplot.show()\n",
    "\n",
    "#     pyplot.plot(acc_train, label='train')\n",
    "#     pyplot.plot(acc_test, label='test')\n",
    "#     pyplot.legend()\n",
    "#     pyplot.show()\n",
    "\n",
    "#     pyplot.plot(loss_train, label='train')\n",
    "#     pyplot.plot(loss_test, label='test')\n",
    "#     pyplot.legend()\n",
    "#     pyplot.show()\n",
    "\n",
    "#     print('Evaluation')\n",
    "#     model.evaluate(testX, testy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
