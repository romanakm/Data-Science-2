{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow intro\n",
    "\n",
    "This tutorial shows the basic usage of tensorflow to train neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NN for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "Read data and convert them to numerical inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=20000, n_features=8, n_informative=5, \n",
    "                           n_redundant=0, n_classes=2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (12800, 8), target_ratio: 0.502\n",
      "test size: (4000, 8), target_ratio: 0.501\n",
      "dev size: (3200, 8), target_ratio: 0.502\n"
     ]
    }
   ],
   "source": [
    "print('train size: {}, target_ratio: {:.3f}'.format(X_train.shape, np.mean(y_train)))\n",
    "print('test size: {}, target_ratio: {:.3f}'.format(X_test.shape, np.mean(y_test)))\n",
    "print('dev size: {}, target_ratio: {:.3f}'.format(X_dev.shape, np.mean(y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple model with tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very useful documentations with many examples and detailed explanation of everything you might need:\n",
    " - https://www.tensorflow.org/api_docs/python/tf/keras/\n",
    " - https://keras.io/api/\n",
    "\n",
    "Contain everything about:\n",
    "  - Model building: Activations, Losses, Optimizers, Regularization\n",
    "  - Data processing\n",
    "  - Pretrained models and datasets\n",
    "  - Automatic differentiation\n",
    "  - ...\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model speficication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "three APIs for building the model\n",
    "   - sequential - easy to code, but less flexible - we will use it sometimes\n",
    "   - functional - flexible and still easy to code - we will use it the most\n",
    "   - model subclassing - rather complicated and not very much used - we will skip it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(https://www.tensorflow.org/guide/keras/sequential_model)\n",
    "\n",
    "Easy to code but <span style=\"color:red\"> NOT </span> appropriate when:\n",
    "\n",
    "- Your model has multiple inputs or multiple outputs\n",
    "- Any of your layers has multiple inputs or multiple outputs\n",
    "- You need to do layer sharing\n",
    "- You want non-linear topology (e.g. a residual connection, a multi-branch model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 10)                90        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Specification A)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer([X_train.shape[1],]), # Create input layer with 'input data' neurons\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"), # Create hidden layer with 10 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"), # Create output layer with one neuron and sigmoid activation\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 10)                90        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Specification B)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(X_train.shape[1],)))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(https://www.tensorflow.org/guide/keras/functional)\n",
    "\n",
    "The Keras functional API is a way to create models that are more flexible than the tf.keras.Sequential API. The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
    "\n",
    "The main idea is that a deep learning model is usually a directed acyclic graph (DAG) of layers. So the functional API is a way to build graphs of layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(X_train.shape[1],))\n",
    "\n",
    "hidden = tf.keras.layers.Dense(10)(inputs)\n",
    "hidden = tf.keras.activations.relu(hidden)\n",
    "hidden = tf.keras.layers.Dense(1)(hidden)\n",
    "outputs = tf.keras.activations.sigmoid(hidden)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                90        \n",
      "                                                                 \n",
      " tf.nn.relu (TFOpLambda)     (None, 10)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      " tf.math.sigmoid (TFOpLambda  (None, 1)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# TODO: To make the following line work you need to install graphviz (if you have not done so in one of the previous classes)\n",
    "# 1) follow the instructions https://graphviz.gitlab.io/download/?fbclid=IwAR1V-lrRhho5rSfBVYXYISsighqRwOCOgMHLmL_DclkQrPtMXQaKj3mFcqs\n",
    "# 2) this notebook has been tested with version 8.0.3\n",
    "# 3) make sure you add it to the PATH variable (you are specifically asked during the installation) at least for local user\n",
    "\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compilation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with selected optimizer, loss and metrics\n",
    "model.compile(\n",
    "        optimizer=tf.optimizers.Adam(), # Several other possibilities for optimizers \n",
    "        loss=tf.losses.BinaryCrossentropy(), # Select the proper loss for the task\n",
    "        metrics=[tf.keras.metrics.AUC(), tf.keras.metrics.BinaryAccuracy()], # Select the proper metrics for the task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Bias of the last layers:\n",
      "[0.]\n",
      "\n",
      ">>> Kernel of the last layers:\n",
      "[[ 0.5154496 ]\n",
      " [-0.44701245]\n",
      " [-0.237315  ]\n",
      " [ 0.01677322]\n",
      " [-0.5168031 ]\n",
      " [ 0.43923312]\n",
      " [-0.02812868]\n",
      " [-0.08891332]\n",
      " [ 0.5554405 ]\n",
      " [ 0.7065024 ]]\n",
      "\n",
      ">>> Bias of the first layers:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      ">>> Kernel of the first layers:\n",
      "[[-0.04693872  0.5302279  -0.40398994 -0.30423754 -0.5357484   0.19146186\n",
      "  -0.5093651   0.39954227  0.20027041  0.1854586 ]\n",
      " [ 0.32080042  0.27005988  0.25124866  0.37265563  0.2149874  -0.03264832\n",
      "   0.5160216   0.09610438  0.17622715 -0.0103789 ]\n",
      " [-0.17918682 -0.5249128   0.27878904 -0.03069329 -0.3362981   0.06663507\n",
      "   0.00683314  0.06641293  0.42232203 -0.46182787]\n",
      " [ 0.5005971   0.55244124 -0.43924788  0.18193102 -0.27093935 -0.0879297\n",
      "  -0.07563651 -0.35239404 -0.52682614  0.57553184]\n",
      " [-0.25627786  0.5265504  -0.21141422  0.19617432 -0.31979886 -0.22778341\n",
      "  -0.11741441  0.57247436  0.35500628  0.22974426]\n",
      " [-0.29636872  0.11769742 -0.33588243  0.30256796  0.29125166  0.53235865\n",
      "   0.06640011  0.01775271  0.1883493   0.05701989]\n",
      " [-0.3973173   0.38746864 -0.30975062 -0.4340152   0.14286357 -0.24289721\n",
      "  -0.56868225 -0.29371467 -0.44650483 -0.18004286]\n",
      " [ 0.46109176 -0.1902022  -0.11706889  0.0780772  -0.1641041  -0.02780563\n",
      "  -0.5106152  -0.56105334 -0.10235909 -0.11173356]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n>>> Bias of the last layers:')\n",
    "print(model.layers[3].weights[1].numpy())\n",
    "\n",
    "print('\\n>>> Kernel of the last layers:')\n",
    "print(model.layers[3].weights[0].numpy())\n",
    "\n",
    "print('\\n>>> Bias of the first layers:')\n",
    "print(model.layers[1].weights[1].numpy())\n",
    "\n",
    "print('\\n>>> Kernel of the first layers:')\n",
    "print(model.layers[1].weights[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 18:15:51.483717: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 383us/step - loss: 0.7562 - auc: 0.5022 - binary_accuracy: 0.5065\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 0s 344us/step - loss: 0.5266 - auc: 0.8279 - binary_accuracy: 0.7354\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 0s 347us/step - loss: 0.4262 - auc: 0.8969 - binary_accuracy: 0.8091\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 0s 348us/step - loss: 0.3760 - auc: 0.9206 - binary_accuracy: 0.8420\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 0s 343us/step - loss: 0.3436 - auc: 0.9343 - binary_accuracy: 0.8648\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 0s 341us/step - loss: 0.3194 - auc: 0.9434 - binary_accuracy: 0.8789\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 0s 344us/step - loss: 0.3011 - auc: 0.9494 - binary_accuracy: 0.8890\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 0s 340us/step - loss: 0.2874 - auc: 0.9530 - binary_accuracy: 0.8937\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 0s 345us/step - loss: 0.2781 - auc: 0.9555 - binary_accuracy: 0.8970\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 0s 333us/step - loss: 0.2705 - auc: 0.9575 - binary_accuracy: 0.8988\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 0s 336us/step - loss: 0.2649 - auc: 0.9589 - binary_accuracy: 0.9016\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 0s 336us/step - loss: 0.2610 - auc: 0.9597 - binary_accuracy: 0.9022\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 0s 351us/step - loss: 0.2574 - auc: 0.9609 - binary_accuracy: 0.9041\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 0s 351us/step - loss: 0.2548 - auc: 0.9616 - binary_accuracy: 0.9055\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 0s 348us/step - loss: 0.2525 - auc: 0.9623 - binary_accuracy: 0.9061\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 0s 352us/step - loss: 0.2504 - auc: 0.9626 - binary_accuracy: 0.9080\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 0s 344us/step - loss: 0.2485 - auc: 0.9633 - binary_accuracy: 0.9080\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 0s 356us/step - loss: 0.2469 - auc: 0.9637 - binary_accuracy: 0.9084\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 0s 356us/step - loss: 0.2454 - auc: 0.9642 - binary_accuracy: 0.9097\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 0s 356us/step - loss: 0.2440 - auc: 0.9646 - binary_accuracy: 0.9101\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 0s 356us/step - loss: 0.2423 - auc: 0.9651 - binary_accuracy: 0.9109\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 0s 354us/step - loss: 0.2409 - auc: 0.9654 - binary_accuracy: 0.9106\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 0s 338us/step - loss: 0.2397 - auc: 0.9660 - binary_accuracy: 0.9104\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 0s 350us/step - loss: 0.2380 - auc: 0.9663 - binary_accuracy: 0.9119\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 0s 337us/step - loss: 0.2367 - auc: 0.9668 - binary_accuracy: 0.9127\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 0s 350us/step - loss: 0.2353 - auc: 0.9672 - binary_accuracy: 0.9125\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 0s 353us/step - loss: 0.2342 - auc: 0.9675 - binary_accuracy: 0.9138\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 0s 353us/step - loss: 0.2331 - auc: 0.9679 - binary_accuracy: 0.9137\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 0s 341us/step - loss: 0.2317 - auc: 0.9683 - binary_accuracy: 0.9136\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 0s 334us/step - loss: 0.2304 - auc: 0.9686 - binary_accuracy: 0.9162\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 0s 340us/step - loss: 0.2295 - auc: 0.9689 - binary_accuracy: 0.9158\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 0s 342us/step - loss: 0.2283 - auc: 0.9693 - binary_accuracy: 0.9166\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 0s 342us/step - loss: 0.2271 - auc: 0.9696 - binary_accuracy: 0.9170\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 0s 348us/step - loss: 0.2261 - auc: 0.9700 - binary_accuracy: 0.9177\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 0s 364us/step - loss: 0.2250 - auc: 0.9701 - binary_accuracy: 0.9181\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 0s 345us/step - loss: 0.2238 - auc: 0.9706 - binary_accuracy: 0.9180\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 0s 338us/step - loss: 0.2228 - auc: 0.9708 - binary_accuracy: 0.9184\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 0s 347us/step - loss: 0.2215 - auc: 0.9712 - binary_accuracy: 0.9192\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 0s 356us/step - loss: 0.2207 - auc: 0.9715 - binary_accuracy: 0.9192\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 0s 348us/step - loss: 0.2194 - auc: 0.9719 - binary_accuracy: 0.9192\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 0s 353us/step - loss: 0.2181 - auc: 0.9721 - binary_accuracy: 0.9205\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 0s 348us/step - loss: 0.2170 - auc: 0.9726 - binary_accuracy: 0.9210\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 0s 342us/step - loss: 0.2156 - auc: 0.9729 - binary_accuracy: 0.9219\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 0s 342us/step - loss: 0.2141 - auc: 0.9733 - binary_accuracy: 0.9226\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 0s 344us/step - loss: 0.2129 - auc: 0.9737 - binary_accuracy: 0.9227\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 0s 344us/step - loss: 0.2112 - auc: 0.9741 - binary_accuracy: 0.9241\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 0s 347us/step - loss: 0.2101 - auc: 0.9744 - binary_accuracy: 0.9247\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 0s 358us/step - loss: 0.2086 - auc: 0.9747 - binary_accuracy: 0.9252\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 0s 345us/step - loss: 0.2070 - auc: 0.9750 - binary_accuracy: 0.9266\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 0s 348us/step - loss: 0.2058 - auc: 0.9753 - binary_accuracy: 0.9263\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 0s 351us/step - loss: 0.2045 - auc: 0.9758 - binary_accuracy: 0.9280\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 0s 345us/step - loss: 0.2032 - auc: 0.9759 - binary_accuracy: 0.9278\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 0s 352us/step - loss: 0.2017 - auc: 0.9763 - binary_accuracy: 0.9296\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 0s 346us/step - loss: 0.2007 - auc: 0.9765 - binary_accuracy: 0.9295\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 0s 346us/step - loss: 0.1994 - auc: 0.9768 - binary_accuracy: 0.9296\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 0s 348us/step - loss: 0.1984 - auc: 0.9770 - binary_accuracy: 0.9302\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 0s 355us/step - loss: 0.1973 - auc: 0.9773 - binary_accuracy: 0.9311\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 0s 357us/step - loss: 0.1962 - auc: 0.9775 - binary_accuracy: 0.9311\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 0s 341us/step - loss: 0.1952 - auc: 0.9777 - binary_accuracy: 0.9314\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 0s 357us/step - loss: 0.1941 - auc: 0.9780 - binary_accuracy: 0.9327\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 0s 373us/step - loss: 0.1935 - auc: 0.9780 - binary_accuracy: 0.9327\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 0s 343us/step - loss: 0.1927 - auc: 0.9782 - binary_accuracy: 0.9332\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 0s 341us/step - loss: 0.1913 - auc: 0.9784 - binary_accuracy: 0.9339\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 0s 341us/step - loss: 0.1909 - auc: 0.9785 - binary_accuracy: 0.9344\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 0s 340us/step - loss: 0.1900 - auc: 0.9786 - binary_accuracy: 0.9339\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 0s 340us/step - loss: 0.1893 - auc: 0.9789 - binary_accuracy: 0.9344\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 0s 341us/step - loss: 0.1880 - auc: 0.9792 - binary_accuracy: 0.9358\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 0s 340us/step - loss: 0.1879 - auc: 0.9792 - binary_accuracy: 0.9351\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 0s 341us/step - loss: 0.1869 - auc: 0.9793 - binary_accuracy: 0.9362\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 0s 342us/step - loss: 0.1862 - auc: 0.9795 - binary_accuracy: 0.9363\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 0s 340us/step - loss: 0.1854 - auc: 0.9796 - binary_accuracy: 0.9363\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 0s 342us/step - loss: 0.1850 - auc: 0.9798 - binary_accuracy: 0.9372\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 0s 341us/step - loss: 0.1842 - auc: 0.9799 - binary_accuracy: 0.9371\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 0s 355us/step - loss: 0.1833 - auc: 0.9800 - binary_accuracy: 0.9377\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 0s 358us/step - loss: 0.1829 - auc: 0.9801 - binary_accuracy: 0.9379\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 0s 350us/step - loss: 0.1819 - auc: 0.9803 - binary_accuracy: 0.9381\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 0s 346us/step - loss: 0.1817 - auc: 0.9804 - binary_accuracy: 0.9391\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 0s 341us/step - loss: 0.1811 - auc: 0.9805 - binary_accuracy: 0.9386\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 0s 342us/step - loss: 0.1804 - auc: 0.9806 - binary_accuracy: 0.9389\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 0s 339us/step - loss: 0.1799 - auc: 0.9807 - binary_accuracy: 0.9391\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 0s 342us/step - loss: 0.1796 - auc: 0.9807 - binary_accuracy: 0.9405\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 0s 346us/step - loss: 0.1785 - auc: 0.9809 - binary_accuracy: 0.9398\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 0s 350us/step - loss: 0.1784 - auc: 0.9810 - binary_accuracy: 0.9410\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 0s 345us/step - loss: 0.1781 - auc: 0.9811 - binary_accuracy: 0.9408\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 0s 348us/step - loss: 0.1775 - auc: 0.9810 - binary_accuracy: 0.9405\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 0s 341us/step - loss: 0.1770 - auc: 0.9812 - binary_accuracy: 0.9402\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 0s 345us/step - loss: 0.1768 - auc: 0.9812 - binary_accuracy: 0.9409\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 0s 340us/step - loss: 0.1759 - auc: 0.9814 - binary_accuracy: 0.9405\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 0s 359us/step - loss: 0.1756 - auc: 0.9814 - binary_accuracy: 0.9417\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 0s 354us/step - loss: 0.1755 - auc: 0.9816 - binary_accuracy: 0.9423\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 0s 352us/step - loss: 0.1749 - auc: 0.9815 - binary_accuracy: 0.9423\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 0s 365us/step - loss: 0.1745 - auc: 0.9816 - binary_accuracy: 0.9420\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 0s 347us/step - loss: 0.1743 - auc: 0.9816 - binary_accuracy: 0.9420\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 0s 343us/step - loss: 0.1737 - auc: 0.9818 - binary_accuracy: 0.9424\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 0s 341us/step - loss: 0.1732 - auc: 0.9818 - binary_accuracy: 0.9422\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 0s 344us/step - loss: 0.1732 - auc: 0.9819 - binary_accuracy: 0.9422\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 0s 349us/step - loss: 0.1725 - auc: 0.9820 - binary_accuracy: 0.9425\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 0s 344us/step - loss: 0.1726 - auc: 0.9819 - binary_accuracy: 0.9422\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 0s 342us/step - loss: 0.1719 - auc: 0.9819 - binary_accuracy: 0.9423\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 0s 348us/step - loss: 0.1719 - auc: 0.9820 - binary_accuracy: 0.9427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15019b9d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model with default setting\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 349us/step - loss: 0.1951 - auc: 0.9771 - binary_accuracy: 0.9348\n",
      "125/125 [==============================] - 0s 246us/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and predict for the test data\n",
    "model.evaluate(X_test, y_test)\n",
    "test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - 0.52\n",
      "0 - 0.32\n",
      "0 - 0.41\n",
      "1 - 1.00\n",
      "0 - 0.00\n",
      "0 - 0.02\n",
      "1 - 0.98\n",
      "0 - 0.00\n",
      "1 - 0.97\n",
      "1 - 1.00\n"
     ]
    }
   ],
   "source": [
    "for pred, true in zip(test_pred, y_test[0:10]):\n",
    "    print('{} - {:.2f}'.format(true, pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add early stopping and regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RegularizedModel\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                90        \n",
      "                                                                 \n",
      " tf.nn.relu_1 (TFOpLambda)   (None, 10)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      " tf.math.sigmoid_1 (TFOpLamb  (None, 1)                0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input layer\n",
    "inputs = tf.keras.Input(shape=(X_train.shape[1]))\n",
    "\n",
    "# Hidden layer with regularization and ReLU\n",
    "hidden = tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(0.001))(inputs)\n",
    "hidden = tf.keras.activations.relu(hidden)\n",
    "\n",
    "# Output layer with regularization and sigmoid\n",
    "outputs = tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.001))(hidden)\n",
    "outputs = tf.keras.activations.sigmoid(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name='RegularizedModel')\n",
    "\n",
    "model.compile(\n",
    "        optimizer=tf.optimizers.Adam(),\n",
    "        loss=tf.losses.BinaryCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.AUC(), tf.keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "400/400 [==============================] - 1s 676us/step - loss: 0.6819 - auc_1: 0.6865 - binary_accuracy: 0.6496 - val_loss: 0.4881 - val_auc_1: 0.8718 - val_binary_accuracy: 0.8047\n",
      "Epoch 2/200\n",
      "400/400 [==============================] - 0s 413us/step - loss: 0.4440 - auc_1: 0.8919 - binary_accuracy: 0.8208 - val_loss: 0.4020 - val_auc_1: 0.9121 - val_binary_accuracy: 0.8366\n",
      "Epoch 3/200\n",
      "400/400 [==============================] - 0s 411us/step - loss: 0.3859 - auc_1: 0.9182 - binary_accuracy: 0.8446 - val_loss: 0.3598 - val_auc_1: 0.9296 - val_binary_accuracy: 0.8631\n",
      "Epoch 4/200\n",
      "400/400 [==============================] - 0s 413us/step - loss: 0.3523 - auc_1: 0.9330 - binary_accuracy: 0.8681 - val_loss: 0.3359 - val_auc_1: 0.9396 - val_binary_accuracy: 0.8813\n",
      "Epoch 5/200\n",
      "400/400 [==============================] - 0s 411us/step - loss: 0.3293 - auc_1: 0.9426 - binary_accuracy: 0.8836 - val_loss: 0.3198 - val_auc_1: 0.9466 - val_binary_accuracy: 0.8900\n",
      "Epoch 6/200\n",
      "400/400 [==============================] - 0s 413us/step - loss: 0.3138 - auc_1: 0.9487 - binary_accuracy: 0.8931 - val_loss: 0.3071 - val_auc_1: 0.9508 - val_binary_accuracy: 0.8925\n",
      "Epoch 7/200\n",
      "400/400 [==============================] - 0s 423us/step - loss: 0.3041 - auc_1: 0.9521 - binary_accuracy: 0.8966 - val_loss: 0.3000 - val_auc_1: 0.9531 - val_binary_accuracy: 0.8959\n",
      "Epoch 8/200\n",
      "400/400 [==============================] - 0s 416us/step - loss: 0.2975 - auc_1: 0.9542 - binary_accuracy: 0.9002 - val_loss: 0.2955 - val_auc_1: 0.9549 - val_binary_accuracy: 0.8950\n",
      "Epoch 9/200\n",
      "400/400 [==============================] - 0s 418us/step - loss: 0.2931 - auc_1: 0.9559 - binary_accuracy: 0.9021 - val_loss: 0.2909 - val_auc_1: 0.9561 - val_binary_accuracy: 0.8988\n",
      "Epoch 10/200\n",
      "400/400 [==============================] - 0s 416us/step - loss: 0.2890 - auc_1: 0.9574 - binary_accuracy: 0.9044 - val_loss: 0.2908 - val_auc_1: 0.9575 - val_binary_accuracy: 0.8953\n",
      "Epoch 11/200\n",
      "400/400 [==============================] - 0s 417us/step - loss: 0.2855 - auc_1: 0.9588 - binary_accuracy: 0.9041 - val_loss: 0.2846 - val_auc_1: 0.9589 - val_binary_accuracy: 0.9006\n",
      "Epoch 12/200\n",
      "400/400 [==============================] - 0s 414us/step - loss: 0.2821 - auc_1: 0.9603 - binary_accuracy: 0.9064 - val_loss: 0.2820 - val_auc_1: 0.9599 - val_binary_accuracy: 0.9000\n",
      "Epoch 13/200\n",
      "400/400 [==============================] - 0s 408us/step - loss: 0.2796 - auc_1: 0.9611 - binary_accuracy: 0.9060 - val_loss: 0.2800 - val_auc_1: 0.9608 - val_binary_accuracy: 0.9016\n",
      "Epoch 14/200\n",
      "400/400 [==============================] - 0s 420us/step - loss: 0.2765 - auc_1: 0.9624 - binary_accuracy: 0.9079 - val_loss: 0.2773 - val_auc_1: 0.9624 - val_binary_accuracy: 0.9009\n",
      "Epoch 15/200\n",
      "400/400 [==============================] - 0s 418us/step - loss: 0.2742 - auc_1: 0.9634 - binary_accuracy: 0.9075 - val_loss: 0.2753 - val_auc_1: 0.9629 - val_binary_accuracy: 0.9041\n",
      "Epoch 16/200\n",
      "400/400 [==============================] - 0s 418us/step - loss: 0.2728 - auc_1: 0.9638 - binary_accuracy: 0.9076 - val_loss: 0.2729 - val_auc_1: 0.9637 - val_binary_accuracy: 0.9019\n",
      "Epoch 17/200\n",
      "400/400 [==============================] - 0s 420us/step - loss: 0.2704 - auc_1: 0.9648 - binary_accuracy: 0.9085 - val_loss: 0.2725 - val_auc_1: 0.9645 - val_binary_accuracy: 0.9028\n",
      "Epoch 18/200\n",
      "400/400 [==============================] - 0s 414us/step - loss: 0.2692 - auc_1: 0.9653 - binary_accuracy: 0.9087 - val_loss: 0.2702 - val_auc_1: 0.9649 - val_binary_accuracy: 0.9034\n",
      "Epoch 19/200\n",
      "400/400 [==============================] - 0s 430us/step - loss: 0.2678 - auc_1: 0.9658 - binary_accuracy: 0.9082 - val_loss: 0.2682 - val_auc_1: 0.9656 - val_binary_accuracy: 0.9075\n",
      "Epoch 20/200\n",
      "400/400 [==============================] - 0s 422us/step - loss: 0.2669 - auc_1: 0.9661 - binary_accuracy: 0.9096 - val_loss: 0.2681 - val_auc_1: 0.9660 - val_binary_accuracy: 0.9059\n",
      "Epoch 21/200\n",
      "400/400 [==============================] - 0s 425us/step - loss: 0.2658 - auc_1: 0.9667 - binary_accuracy: 0.9098 - val_loss: 0.2657 - val_auc_1: 0.9665 - val_binary_accuracy: 0.9062\n",
      "Epoch 22/200\n",
      "400/400 [==============================] - 0s 425us/step - loss: 0.2646 - auc_1: 0.9671 - binary_accuracy: 0.9112 - val_loss: 0.2659 - val_auc_1: 0.9668 - val_binary_accuracy: 0.9078\n",
      "Epoch 23/200\n",
      "400/400 [==============================] - 0s 416us/step - loss: 0.2641 - auc_1: 0.9674 - binary_accuracy: 0.9124 - val_loss: 0.2640 - val_auc_1: 0.9673 - val_binary_accuracy: 0.9078\n",
      "Epoch 24/200\n",
      "400/400 [==============================] - 0s 420us/step - loss: 0.2632 - auc_1: 0.9676 - binary_accuracy: 0.9112 - val_loss: 0.2639 - val_auc_1: 0.9678 - val_binary_accuracy: 0.9094\n",
      "Epoch 25/200\n",
      "400/400 [==============================] - 0s 417us/step - loss: 0.2625 - auc_1: 0.9681 - binary_accuracy: 0.9130 - val_loss: 0.2623 - val_auc_1: 0.9680 - val_binary_accuracy: 0.9097\n",
      "Epoch 26/200\n",
      "400/400 [==============================] - 0s 419us/step - loss: 0.2622 - auc_1: 0.9682 - binary_accuracy: 0.9117 - val_loss: 0.2619 - val_auc_1: 0.9682 - val_binary_accuracy: 0.9091\n",
      "Epoch 27/200\n",
      "400/400 [==============================] - 0s 422us/step - loss: 0.2613 - auc_1: 0.9685 - binary_accuracy: 0.9143 - val_loss: 0.2609 - val_auc_1: 0.9689 - val_binary_accuracy: 0.9106\n",
      "Epoch 28/200\n",
      "400/400 [==============================] - 0s 422us/step - loss: 0.2608 - auc_1: 0.9689 - binary_accuracy: 0.9153 - val_loss: 0.2617 - val_auc_1: 0.9688 - val_binary_accuracy: 0.9094\n",
      "Epoch 29/200\n",
      "400/400 [==============================] - 0s 419us/step - loss: 0.2602 - auc_1: 0.9690 - binary_accuracy: 0.9144 - val_loss: 0.2601 - val_auc_1: 0.9690 - val_binary_accuracy: 0.9116\n",
      "Epoch 30/200\n",
      "400/400 [==============================] - 0s 419us/step - loss: 0.2593 - auc_1: 0.9694 - binary_accuracy: 0.9129 - val_loss: 0.2586 - val_auc_1: 0.9699 - val_binary_accuracy: 0.9116\n",
      "Epoch 31/200\n",
      "400/400 [==============================] - 0s 419us/step - loss: 0.2582 - auc_1: 0.9700 - binary_accuracy: 0.9152 - val_loss: 0.2576 - val_auc_1: 0.9704 - val_binary_accuracy: 0.9144\n",
      "Epoch 32/200\n",
      "400/400 [==============================] - 0s 421us/step - loss: 0.2573 - auc_1: 0.9705 - binary_accuracy: 0.9159 - val_loss: 0.2560 - val_auc_1: 0.9709 - val_binary_accuracy: 0.9150\n",
      "Epoch 33/200\n",
      "400/400 [==============================] - 0s 427us/step - loss: 0.2561 - auc_1: 0.9709 - binary_accuracy: 0.9179 - val_loss: 0.2545 - val_auc_1: 0.9714 - val_binary_accuracy: 0.9159\n",
      "Epoch 34/200\n",
      "400/400 [==============================] - 0s 420us/step - loss: 0.2547 - auc_1: 0.9715 - binary_accuracy: 0.9180 - val_loss: 0.2538 - val_auc_1: 0.9717 - val_binary_accuracy: 0.9178\n",
      "Epoch 35/200\n",
      "400/400 [==============================] - 0s 417us/step - loss: 0.2532 - auc_1: 0.9721 - binary_accuracy: 0.9185 - val_loss: 0.2515 - val_auc_1: 0.9720 - val_binary_accuracy: 0.9187\n",
      "Epoch 36/200\n",
      "400/400 [==============================] - 0s 413us/step - loss: 0.2522 - auc_1: 0.9723 - binary_accuracy: 0.9202 - val_loss: 0.2509 - val_auc_1: 0.9727 - val_binary_accuracy: 0.9187\n",
      "Epoch 37/200\n",
      "400/400 [==============================] - 0s 414us/step - loss: 0.2510 - auc_1: 0.9727 - binary_accuracy: 0.9200 - val_loss: 0.2506 - val_auc_1: 0.9735 - val_binary_accuracy: 0.9216\n",
      "Epoch 38/200\n",
      "400/400 [==============================] - 0s 422us/step - loss: 0.2502 - auc_1: 0.9732 - binary_accuracy: 0.9214 - val_loss: 0.2490 - val_auc_1: 0.9730 - val_binary_accuracy: 0.9191\n",
      "Epoch 39/200\n",
      "400/400 [==============================] - 0s 421us/step - loss: 0.2493 - auc_1: 0.9734 - binary_accuracy: 0.9215 - val_loss: 0.2501 - val_auc_1: 0.9738 - val_binary_accuracy: 0.9178\n",
      "Epoch 40/200\n",
      "400/400 [==============================] - 0s 422us/step - loss: 0.2486 - auc_1: 0.9738 - binary_accuracy: 0.9222 - val_loss: 0.2476 - val_auc_1: 0.9736 - val_binary_accuracy: 0.9228\n",
      "Epoch 41/200\n",
      "400/400 [==============================] - 0s 421us/step - loss: 0.2479 - auc_1: 0.9739 - binary_accuracy: 0.9243 - val_loss: 0.2481 - val_auc_1: 0.9741 - val_binary_accuracy: 0.9187\n",
      "Epoch 42/200\n",
      "400/400 [==============================] - 0s 417us/step - loss: 0.2473 - auc_1: 0.9742 - binary_accuracy: 0.9225 - val_loss: 0.2468 - val_auc_1: 0.9743 - val_binary_accuracy: 0.9212\n",
      "Epoch 43/200\n",
      "400/400 [==============================] - 0s 414us/step - loss: 0.2465 - auc_1: 0.9744 - binary_accuracy: 0.9248 - val_loss: 0.2460 - val_auc_1: 0.9743 - val_binary_accuracy: 0.9225\n",
      "Epoch 44/200\n",
      "400/400 [==============================] - 0s 418us/step - loss: 0.2464 - auc_1: 0.9745 - binary_accuracy: 0.9246 - val_loss: 0.2462 - val_auc_1: 0.9749 - val_binary_accuracy: 0.9216\n",
      "Epoch 45/200\n",
      "400/400 [==============================] - 0s 430us/step - loss: 0.2457 - auc_1: 0.9747 - binary_accuracy: 0.9248 - val_loss: 0.2451 - val_auc_1: 0.9750 - val_binary_accuracy: 0.9219\n",
      "Epoch 46/200\n",
      "400/400 [==============================] - 0s 482us/step - loss: 0.2451 - auc_1: 0.9750 - binary_accuracy: 0.9244 - val_loss: 0.2445 - val_auc_1: 0.9747 - val_binary_accuracy: 0.9228\n",
      "Epoch 47/200\n",
      "400/400 [==============================] - 0s 421us/step - loss: 0.2448 - auc_1: 0.9751 - binary_accuracy: 0.9245 - val_loss: 0.2436 - val_auc_1: 0.9753 - val_binary_accuracy: 0.9231\n",
      "Epoch 48/200\n",
      "400/400 [==============================] - 0s 421us/step - loss: 0.2446 - auc_1: 0.9753 - binary_accuracy: 0.9248 - val_loss: 0.2432 - val_auc_1: 0.9752 - val_binary_accuracy: 0.9247\n",
      "Epoch 49/200\n",
      "400/400 [==============================] - 0s 420us/step - loss: 0.2441 - auc_1: 0.9753 - binary_accuracy: 0.9266 - val_loss: 0.2431 - val_auc_1: 0.9754 - val_binary_accuracy: 0.9241\n",
      "Epoch 50/200\n",
      "400/400 [==============================] - 0s 416us/step - loss: 0.2436 - auc_1: 0.9755 - binary_accuracy: 0.9253 - val_loss: 0.2430 - val_auc_1: 0.9759 - val_binary_accuracy: 0.9237\n",
      "Epoch 51/200\n",
      "400/400 [==============================] - 0s 411us/step - loss: 0.2433 - auc_1: 0.9758 - binary_accuracy: 0.9270 - val_loss: 0.2434 - val_auc_1: 0.9754 - val_binary_accuracy: 0.9234\n",
      "Epoch 52/200\n",
      "400/400 [==============================] - 0s 421us/step - loss: 0.2430 - auc_1: 0.9757 - binary_accuracy: 0.9273 - val_loss: 0.2431 - val_auc_1: 0.9755 - val_binary_accuracy: 0.9237\n",
      "Epoch 53/200\n",
      "400/400 [==============================] - 0s 418us/step - loss: 0.2427 - auc_1: 0.9758 - binary_accuracy: 0.9282 - val_loss: 0.2432 - val_auc_1: 0.9757 - val_binary_accuracy: 0.9225\n",
      "Epoch 54/200\n",
      "400/400 [==============================] - 0s 420us/step - loss: 0.2422 - auc_1: 0.9761 - binary_accuracy: 0.9277 - val_loss: 0.2426 - val_auc_1: 0.9760 - val_binary_accuracy: 0.9253\n",
      "Epoch 55/200\n",
      "400/400 [==============================] - 0s 433us/step - loss: 0.2422 - auc_1: 0.9761 - binary_accuracy: 0.9275 - val_loss: 0.2422 - val_auc_1: 0.9762 - val_binary_accuracy: 0.9237\n",
      "Epoch 56/200\n",
      "400/400 [==============================] - 0s 420us/step - loss: 0.2421 - auc_1: 0.9761 - binary_accuracy: 0.9272 - val_loss: 0.2421 - val_auc_1: 0.9764 - val_binary_accuracy: 0.9250\n",
      "Epoch 57/200\n",
      "400/400 [==============================] - 0s 415us/step - loss: 0.2417 - auc_1: 0.9763 - binary_accuracy: 0.9280 - val_loss: 0.2414 - val_auc_1: 0.9760 - val_binary_accuracy: 0.9262\n",
      "Epoch 58/200\n",
      "400/400 [==============================] - 0s 423us/step - loss: 0.2410 - auc_1: 0.9766 - binary_accuracy: 0.9287 - val_loss: 0.2421 - val_auc_1: 0.9758 - val_binary_accuracy: 0.9262\n",
      "Epoch 59/200\n",
      "400/400 [==============================] - 0s 422us/step - loss: 0.2415 - auc_1: 0.9763 - binary_accuracy: 0.9275 - val_loss: 0.2406 - val_auc_1: 0.9767 - val_binary_accuracy: 0.9275\n",
      "Epoch 60/200\n",
      "400/400 [==============================] - 0s 419us/step - loss: 0.2411 - auc_1: 0.9765 - binary_accuracy: 0.9280 - val_loss: 0.2401 - val_auc_1: 0.9770 - val_binary_accuracy: 0.9287\n",
      "Epoch 61/200\n",
      "400/400 [==============================] - 0s 421us/step - loss: 0.2405 - auc_1: 0.9768 - binary_accuracy: 0.9294 - val_loss: 0.2409 - val_auc_1: 0.9767 - val_binary_accuracy: 0.9269\n",
      "Epoch 62/200\n",
      "400/400 [==============================] - 0s 420us/step - loss: 0.2403 - auc_1: 0.9768 - binary_accuracy: 0.9296 - val_loss: 0.2404 - val_auc_1: 0.9767 - val_binary_accuracy: 0.9272\n",
      "Epoch 63/200\n",
      "400/400 [==============================] - 0s 416us/step - loss: 0.2400 - auc_1: 0.9769 - binary_accuracy: 0.9301 - val_loss: 0.2388 - val_auc_1: 0.9773 - val_binary_accuracy: 0.9275\n",
      "Epoch 64/200\n",
      "400/400 [==============================] - 0s 414us/step - loss: 0.2398 - auc_1: 0.9771 - binary_accuracy: 0.9292 - val_loss: 0.2390 - val_auc_1: 0.9772 - val_binary_accuracy: 0.9319\n",
      "Epoch 65/200\n",
      "400/400 [==============================] - 0s 417us/step - loss: 0.2396 - auc_1: 0.9772 - binary_accuracy: 0.9295 - val_loss: 0.2397 - val_auc_1: 0.9774 - val_binary_accuracy: 0.9303\n",
      "Epoch 66/200\n",
      "400/400 [==============================] - 0s 423us/step - loss: 0.2394 - auc_1: 0.9773 - binary_accuracy: 0.9299 - val_loss: 0.2379 - val_auc_1: 0.9772 - val_binary_accuracy: 0.9309\n",
      "Epoch 67/200\n",
      "400/400 [==============================] - 0s 418us/step - loss: 0.2392 - auc_1: 0.9773 - binary_accuracy: 0.9314 - val_loss: 0.2395 - val_auc_1: 0.9771 - val_binary_accuracy: 0.9275\n",
      "Epoch 68/200\n",
      "400/400 [==============================] - 0s 416us/step - loss: 0.2392 - auc_1: 0.9773 - binary_accuracy: 0.9294 - val_loss: 0.2376 - val_auc_1: 0.9775 - val_binary_accuracy: 0.9312\n",
      "Epoch 69/200\n",
      "400/400 [==============================] - 0s 414us/step - loss: 0.2386 - auc_1: 0.9776 - binary_accuracy: 0.9309 - val_loss: 0.2387 - val_auc_1: 0.9778 - val_binary_accuracy: 0.9306\n",
      "Epoch 70/200\n",
      "400/400 [==============================] - 0s 415us/step - loss: 0.2384 - auc_1: 0.9777 - binary_accuracy: 0.9312 - val_loss: 0.2378 - val_auc_1: 0.9773 - val_binary_accuracy: 0.9322\n",
      "Epoch 71/200\n",
      "400/400 [==============================] - 0s 416us/step - loss: 0.2384 - auc_1: 0.9776 - binary_accuracy: 0.9312 - val_loss: 0.2372 - val_auc_1: 0.9779 - val_binary_accuracy: 0.9281\n",
      "Epoch 72/200\n",
      "400/400 [==============================] - 0s 416us/step - loss: 0.2378 - auc_1: 0.9778 - binary_accuracy: 0.9309 - val_loss: 0.2373 - val_auc_1: 0.9780 - val_binary_accuracy: 0.9309\n",
      "Epoch 73/200\n",
      "400/400 [==============================] - 0s 416us/step - loss: 0.2378 - auc_1: 0.9779 - binary_accuracy: 0.9311 - val_loss: 0.2365 - val_auc_1: 0.9777 - val_binary_accuracy: 0.9322\n",
      "Epoch 74/200\n",
      "400/400 [==============================] - 0s 421us/step - loss: 0.2377 - auc_1: 0.9778 - binary_accuracy: 0.9318 - val_loss: 0.2368 - val_auc_1: 0.9782 - val_binary_accuracy: 0.9328\n",
      "Epoch 75/200\n",
      "400/400 [==============================] - 0s 423us/step - loss: 0.2371 - auc_1: 0.9781 - binary_accuracy: 0.9316 - val_loss: 0.2369 - val_auc_1: 0.9783 - val_binary_accuracy: 0.9331\n",
      "Epoch 76/200\n",
      "400/400 [==============================] - 0s 427us/step - loss: 0.2371 - auc_1: 0.9781 - binary_accuracy: 0.9322 - val_loss: 0.2366 - val_auc_1: 0.9782 - val_binary_accuracy: 0.9328\n",
      "Epoch 77/200\n",
      "400/400 [==============================] - 0s 421us/step - loss: 0.2373 - auc_1: 0.9782 - binary_accuracy: 0.9316 - val_loss: 0.2360 - val_auc_1: 0.9782 - val_binary_accuracy: 0.9325\n",
      "Epoch 78/200\n",
      "400/400 [==============================] - 0s 419us/step - loss: 0.2368 - auc_1: 0.9782 - binary_accuracy: 0.9328 - val_loss: 0.2362 - val_auc_1: 0.9781 - val_binary_accuracy: 0.9300\n",
      "Epoch 79/200\n",
      "400/400 [==============================] - 0s 415us/step - loss: 0.2364 - auc_1: 0.9782 - binary_accuracy: 0.9320 - val_loss: 0.2374 - val_auc_1: 0.9787 - val_binary_accuracy: 0.9325\n",
      "Epoch 80/200\n",
      "400/400 [==============================] - 0s 416us/step - loss: 0.2360 - auc_1: 0.9785 - binary_accuracy: 0.9323 - val_loss: 0.2371 - val_auc_1: 0.9786 - val_binary_accuracy: 0.9334\n",
      "Epoch 81/200\n",
      "400/400 [==============================] - 0s 426us/step - loss: 0.2363 - auc_1: 0.9785 - binary_accuracy: 0.9324 - val_loss: 0.2352 - val_auc_1: 0.9786 - val_binary_accuracy: 0.9334\n",
      "Epoch 82/200\n",
      "400/400 [==============================] - 0s 419us/step - loss: 0.2359 - auc_1: 0.9786 - binary_accuracy: 0.9335 - val_loss: 0.2358 - val_auc_1: 0.9789 - val_binary_accuracy: 0.9369\n",
      "Epoch 83/200\n",
      "400/400 [==============================] - 0s 415us/step - loss: 0.2354 - auc_1: 0.9788 - binary_accuracy: 0.9334 - val_loss: 0.2357 - val_auc_1: 0.9785 - val_binary_accuracy: 0.9341\n",
      "Epoch 84/200\n",
      "400/400 [==============================] - 0s 415us/step - loss: 0.2355 - auc_1: 0.9788 - binary_accuracy: 0.9333 - val_loss: 0.2346 - val_auc_1: 0.9789 - val_binary_accuracy: 0.9353\n",
      "Epoch 85/200\n",
      "400/400 [==============================] - 0s 415us/step - loss: 0.2350 - auc_1: 0.9788 - binary_accuracy: 0.9331 - val_loss: 0.2355 - val_auc_1: 0.9789 - val_binary_accuracy: 0.9356\n",
      "Epoch 86/200\n",
      "400/400 [==============================] - 0s 425us/step - loss: 0.2349 - auc_1: 0.9791 - binary_accuracy: 0.9336 - val_loss: 0.2348 - val_auc_1: 0.9782 - val_binary_accuracy: 0.9328\n",
      "Epoch 87/200\n",
      "400/400 [==============================] - 0s 429us/step - loss: 0.2348 - auc_1: 0.9788 - binary_accuracy: 0.9339 - val_loss: 0.2363 - val_auc_1: 0.9788 - val_binary_accuracy: 0.9309\n",
      "Epoch 88/200\n",
      "400/400 [==============================] - 0s 415us/step - loss: 0.2348 - auc_1: 0.9791 - binary_accuracy: 0.9334 - val_loss: 0.2340 - val_auc_1: 0.9790 - val_binary_accuracy: 0.9350\n",
      "Epoch 89/200\n",
      "400/400 [==============================] - 0s 416us/step - loss: 0.2341 - auc_1: 0.9791 - binary_accuracy: 0.9330 - val_loss: 0.2339 - val_auc_1: 0.9792 - val_binary_accuracy: 0.9356\n",
      "Epoch 90/200\n",
      "400/400 [==============================] - 0s 413us/step - loss: 0.2341 - auc_1: 0.9794 - binary_accuracy: 0.9347 - val_loss: 0.2345 - val_auc_1: 0.9787 - val_binary_accuracy: 0.9344\n",
      "Epoch 91/200\n",
      "400/400 [==============================] - 0s 415us/step - loss: 0.2338 - auc_1: 0.9792 - binary_accuracy: 0.9337 - val_loss: 0.2339 - val_auc_1: 0.9794 - val_binary_accuracy: 0.9362\n",
      "Epoch 92/200\n",
      "400/400 [==============================] - 0s 415us/step - loss: 0.2335 - auc_1: 0.9794 - binary_accuracy: 0.9341 - val_loss: 0.2339 - val_auc_1: 0.9793 - val_binary_accuracy: 0.9353\n",
      "Epoch 93/200\n",
      "400/400 [==============================] - 0s 767us/step - loss: 0.2333 - auc_1: 0.9795 - binary_accuracy: 0.9343 - val_loss: 0.2338 - val_auc_1: 0.9795 - val_binary_accuracy: 0.9337\n",
      "Epoch 94/200\n",
      "400/400 [==============================] - 0s 415us/step - loss: 0.2332 - auc_1: 0.9795 - binary_accuracy: 0.9341 - val_loss: 0.2334 - val_auc_1: 0.9795 - val_binary_accuracy: 0.9356\n",
      "Epoch 95/200\n",
      "400/400 [==============================] - 0s 415us/step - loss: 0.2328 - auc_1: 0.9796 - binary_accuracy: 0.9341 - val_loss: 0.2339 - val_auc_1: 0.9795 - val_binary_accuracy: 0.9362\n",
      "Epoch 96/200\n",
      "400/400 [==============================] - 0s 416us/step - loss: 0.2329 - auc_1: 0.9797 - binary_accuracy: 0.9330 - val_loss: 0.2331 - val_auc_1: 0.9795 - val_binary_accuracy: 0.9356\n",
      "Epoch 97/200\n",
      "400/400 [==============================] - 0s 425us/step - loss: 0.2327 - auc_1: 0.9798 - binary_accuracy: 0.9362 - val_loss: 0.2332 - val_auc_1: 0.9790 - val_binary_accuracy: 0.9341\n",
      "Epoch 98/200\n",
      "400/400 [==============================] - 0s 427us/step - loss: 0.2323 - auc_1: 0.9797 - binary_accuracy: 0.9348 - val_loss: 0.2336 - val_auc_1: 0.9794 - val_binary_accuracy: 0.9375\n",
      "Epoch 99/200\n",
      "400/400 [==============================] - 0s 416us/step - loss: 0.2323 - auc_1: 0.9798 - binary_accuracy: 0.9355 - val_loss: 0.2334 - val_auc_1: 0.9797 - val_binary_accuracy: 0.9372\n",
      "Epoch 100/200\n",
      "400/400 [==============================] - 0s 419us/step - loss: 0.2321 - auc_1: 0.9801 - binary_accuracy: 0.9356 - val_loss: 0.2332 - val_auc_1: 0.9794 - val_binary_accuracy: 0.9372\n",
      "Epoch 101/200\n",
      "400/400 [==============================] - 0s 416us/step - loss: 0.2320 - auc_1: 0.9800 - binary_accuracy: 0.9352 - val_loss: 0.2321 - val_auc_1: 0.9796 - val_binary_accuracy: 0.9384\n",
      "Epoch 102/200\n",
      "400/400 [==============================] - 0s 415us/step - loss: 0.2319 - auc_1: 0.9799 - binary_accuracy: 0.9348 - val_loss: 0.2330 - val_auc_1: 0.9793 - val_binary_accuracy: 0.9372\n",
      "Epoch 103/200\n",
      "400/400 [==============================] - 0s 414us/step - loss: 0.2320 - auc_1: 0.9800 - binary_accuracy: 0.9354 - val_loss: 0.2334 - val_auc_1: 0.9793 - val_binary_accuracy: 0.9369\n",
      "Epoch 104/200\n",
      "400/400 [==============================] - 0s 415us/step - loss: 0.2316 - auc_1: 0.9801 - binary_accuracy: 0.9360 - val_loss: 0.2326 - val_auc_1: 0.9798 - val_binary_accuracy: 0.9384\n",
      "Epoch 105/200\n",
      "400/400 [==============================] - 0s 422us/step - loss: 0.2312 - auc_1: 0.9801 - binary_accuracy: 0.9362 - val_loss: 0.2327 - val_auc_1: 0.9798 - val_binary_accuracy: 0.9369\n",
      "Epoch 106/200\n",
      "400/400 [==============================] - 0s 422us/step - loss: 0.2312 - auc_1: 0.9802 - binary_accuracy: 0.9356 - val_loss: 0.2323 - val_auc_1: 0.9801 - val_binary_accuracy: 0.9394\n",
      "Epoch 107/200\n",
      "400/400 [==============================] - 0s 420us/step - loss: 0.2312 - auc_1: 0.9803 - binary_accuracy: 0.9366 - val_loss: 0.2322 - val_auc_1: 0.9795 - val_binary_accuracy: 0.9378\n",
      "Epoch 108/200\n",
      "400/400 [==============================] - 0s 439us/step - loss: 0.2309 - auc_1: 0.9804 - binary_accuracy: 0.9349 - val_loss: 0.2319 - val_auc_1: 0.9797 - val_binary_accuracy: 0.9375\n",
      "Epoch 109/200\n",
      "400/400 [==============================] - 0s 421us/step - loss: 0.2305 - auc_1: 0.9806 - binary_accuracy: 0.9362 - val_loss: 0.2316 - val_auc_1: 0.9798 - val_binary_accuracy: 0.9366\n",
      "Epoch 110/200\n",
      "400/400 [==============================] - 0s 416us/step - loss: 0.2305 - auc_1: 0.9805 - binary_accuracy: 0.9354 - val_loss: 0.2353 - val_auc_1: 0.9795 - val_binary_accuracy: 0.9381\n",
      "Epoch 111/200\n",
      "400/400 [==============================] - 0s 416us/step - loss: 0.2309 - auc_1: 0.9804 - binary_accuracy: 0.9361 - val_loss: 0.2322 - val_auc_1: 0.9799 - val_binary_accuracy: 0.9381\n",
      "Epoch 112/200\n",
      "400/400 [==============================] - 0s 420us/step - loss: 0.2305 - auc_1: 0.9805 - binary_accuracy: 0.9359 - val_loss: 0.2317 - val_auc_1: 0.9802 - val_binary_accuracy: 0.9388\n",
      "Epoch 113/200\n",
      "400/400 [==============================] - 0s 421us/step - loss: 0.2307 - auc_1: 0.9805 - binary_accuracy: 0.9364 - val_loss: 0.2319 - val_auc_1: 0.9796 - val_binary_accuracy: 0.9375\n",
      "Epoch 114/200\n",
      "400/400 [==============================] - 0s 420us/step - loss: 0.2301 - auc_1: 0.9806 - binary_accuracy: 0.9362 - val_loss: 0.2311 - val_auc_1: 0.9804 - val_binary_accuracy: 0.9384\n",
      "Epoch 115/200\n",
      "400/400 [==============================] - 0s 419us/step - loss: 0.2304 - auc_1: 0.9806 - binary_accuracy: 0.9362 - val_loss: 0.2306 - val_auc_1: 0.9803 - val_binary_accuracy: 0.9381\n",
      "Epoch 116/200\n",
      "400/400 [==============================] - 0s 427us/step - loss: 0.2300 - auc_1: 0.9808 - binary_accuracy: 0.9369 - val_loss: 0.2316 - val_auc_1: 0.9799 - val_binary_accuracy: 0.9369\n",
      "Epoch 117/200\n",
      "400/400 [==============================] - 0s 420us/step - loss: 0.2301 - auc_1: 0.9805 - binary_accuracy: 0.9355 - val_loss: 0.2316 - val_auc_1: 0.9803 - val_binary_accuracy: 0.9400\n",
      "Epoch 118/200\n",
      "400/400 [==============================] - 0s 415us/step - loss: 0.2298 - auc_1: 0.9808 - binary_accuracy: 0.9357 - val_loss: 0.2327 - val_auc_1: 0.9801 - val_binary_accuracy: 0.9378\n",
      "Epoch 119/200\n",
      "400/400 [==============================] - 0s 415us/step - loss: 0.2295 - auc_1: 0.9809 - binary_accuracy: 0.9362 - val_loss: 0.2332 - val_auc_1: 0.9805 - val_binary_accuracy: 0.9406\n",
      "Epoch 120/200\n",
      "400/400 [==============================] - 0s 427us/step - loss: 0.2297 - auc_1: 0.9809 - binary_accuracy: 0.9366 - val_loss: 0.2309 - val_auc_1: 0.9803 - val_binary_accuracy: 0.9391\n",
      "Epoch 121/200\n",
      "400/400 [==============================] - 0s 427us/step - loss: 0.2298 - auc_1: 0.9808 - binary_accuracy: 0.9370 - val_loss: 0.2309 - val_auc_1: 0.9806 - val_binary_accuracy: 0.9394\n",
      "Epoch 122/200\n",
      "400/400 [==============================] - 0s 423us/step - loss: 0.2294 - auc_1: 0.9809 - binary_accuracy: 0.9364 - val_loss: 0.2313 - val_auc_1: 0.9805 - val_binary_accuracy: 0.9416\n",
      "Epoch 123/200\n",
      "400/400 [==============================] - 0s 422us/step - loss: 0.2295 - auc_1: 0.9811 - binary_accuracy: 0.9376 - val_loss: 0.2307 - val_auc_1: 0.9807 - val_binary_accuracy: 0.9400\n",
      "Epoch 124/200\n",
      "400/400 [==============================] - 0s 423us/step - loss: 0.2288 - auc_1: 0.9811 - binary_accuracy: 0.9376 - val_loss: 0.2339 - val_auc_1: 0.9805 - val_binary_accuracy: 0.9372\n",
      "Epoch 125/200\n",
      "400/400 [==============================] - 0s 421us/step - loss: 0.2290 - auc_1: 0.9810 - binary_accuracy: 0.9370 - val_loss: 0.2312 - val_auc_1: 0.9807 - val_binary_accuracy: 0.9391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1511e0700>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "early_call = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_dev, y_dev),\n",
    "          callbacks=[early_call])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch and Tensorboard\n",
    "Run gridsearch over hidden layer size, L2 regularization, activation, check the outputs in Tensorboard\n",
    "\n",
    "I recommend not to run Tensorboard from Jupyter notebook but from terminal directly\n",
    "\n",
    "use \"tensorboard --logdir logs\" in command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 1s 1ms/step - loss: 1.0894 - AUC: 0.3901 - binary_accuracy: 0.4209 - val_loss: 0.9411 - val_AUC: 0.3918 - val_binary_accuracy: 0.4253\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 0s 478us/step - loss: 0.8571 - AUC: 0.4234 - binary_accuracy: 0.4494 - val_loss: 0.7966 - val_AUC: 0.4360 - val_binary_accuracy: 0.4628\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 0s 480us/step - loss: 0.7598 - AUC: 0.4681 - binary_accuracy: 0.4938 - val_loss: 0.7302 - val_AUC: 0.4953 - val_binary_accuracy: 0.5172\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 0s 481us/step - loss: 0.7084 - AUC: 0.5268 - binary_accuracy: 0.5459 - val_loss: 0.6892 - val_AUC: 0.5620 - val_binary_accuracy: 0.5775\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 0s 487us/step - loss: 0.6731 - AUC: 0.5926 - binary_accuracy: 0.6101 - val_loss: 0.6579 - val_AUC: 0.6327 - val_binary_accuracy: 0.6422\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 0s 464us/step - loss: 0.6476 - AUC: 0.6504 - binary_accuracy: 0.6584 - val_loss: 0.6366 - val_AUC: 0.6771 - val_binary_accuracy: 0.6756\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 0s 481us/step - loss: 0.6324 - AUC: 0.6810 - binary_accuracy: 0.6733 - val_loss: 0.6243 - val_AUC: 0.6988 - val_binary_accuracy: 0.6881\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 0s 478us/step - loss: 0.6233 - AUC: 0.6940 - binary_accuracy: 0.6771 - val_loss: 0.6160 - val_AUC: 0.7103 - val_binary_accuracy: 0.6897\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 0s 457us/step - loss: 0.6167 - AUC: 0.6981 - binary_accuracy: 0.6790 - val_loss: 0.6094 - val_AUC: 0.7187 - val_binary_accuracy: 0.6906\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 0s 454us/step - loss: 0.6117 - AUC: 0.7033 - binary_accuracy: 0.6819 - val_loss: 0.6043 - val_AUC: 0.7216 - val_binary_accuracy: 0.6972\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 0s 981us/step - loss: 0.8906 - AUC: 0.5738 - binary_accuracy: 0.5052 - val_loss: 0.7404 - val_AUC: 0.6634 - val_binary_accuracy: 0.5406\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 0s 465us/step - loss: 0.6820 - AUC: 0.7365 - binary_accuracy: 0.6089 - val_loss: 0.6330 - val_AUC: 0.8052 - val_binary_accuracy: 0.6872\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 0s 492us/step - loss: 0.6104 - AUC: 0.8263 - binary_accuracy: 0.7381 - val_loss: 0.5842 - val_AUC: 0.8541 - val_binary_accuracy: 0.7741\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 0s 477us/step - loss: 0.5754 - AUC: 0.8537 - binary_accuracy: 0.7877 - val_loss: 0.5574 - val_AUC: 0.8699 - val_binary_accuracy: 0.8000\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 0s 477us/step - loss: 0.5549 - AUC: 0.8658 - binary_accuracy: 0.7996 - val_loss: 0.5410 - val_AUC: 0.8770 - val_binary_accuracy: 0.8041\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 0s 493us/step - loss: 0.5413 - AUC: 0.8710 - binary_accuracy: 0.8030 - val_loss: 0.5298 - val_AUC: 0.8802 - val_binary_accuracy: 0.8072\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 0s 469us/step - loss: 0.5318 - AUC: 0.8730 - binary_accuracy: 0.8027 - val_loss: 0.5220 - val_AUC: 0.8811 - val_binary_accuracy: 0.8097\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 0s 472us/step - loss: 0.5249 - AUC: 0.8741 - binary_accuracy: 0.8024 - val_loss: 0.5159 - val_AUC: 0.8838 - val_binary_accuracy: 0.8056\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 0s 467us/step - loss: 0.5192 - AUC: 0.8770 - binary_accuracy: 0.8041 - val_loss: 0.5105 - val_AUC: 0.8849 - val_binary_accuracy: 0.8078\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 0s 473us/step - loss: 0.5141 - AUC: 0.8793 - binary_accuracy: 0.8048 - val_loss: 0.5056 - val_AUC: 0.8860 - val_binary_accuracy: 0.8091\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 1s 1ms/step - loss: 0.9172 - AUC: 0.3765 - binary_accuracy: 0.4213 - val_loss: 0.8395 - val_AUC: 0.4283 - val_binary_accuracy: 0.4519\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 0s 470us/step - loss: 0.7690 - AUC: 0.4995 - binary_accuracy: 0.5135 - val_loss: 0.7171 - val_AUC: 0.5765 - val_binary_accuracy: 0.5544\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 0s 472us/step - loss: 0.6691 - AUC: 0.6673 - binary_accuracy: 0.6247 - val_loss: 0.6298 - val_AUC: 0.7568 - val_binary_accuracy: 0.6822\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 0s 471us/step - loss: 0.5987 - AUC: 0.8043 - binary_accuracy: 0.6998 - val_loss: 0.5673 - val_AUC: 0.8475 - val_binary_accuracy: 0.7578\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 0s 474us/step - loss: 0.5514 - AUC: 0.8488 - binary_accuracy: 0.7588 - val_loss: 0.5269 - val_AUC: 0.8690 - val_binary_accuracy: 0.7941\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 0s 468us/step - loss: 0.5243 - AUC: 0.8605 - binary_accuracy: 0.7779 - val_loss: 0.5061 - val_AUC: 0.8748 - val_binary_accuracy: 0.8037\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 0s 468us/step - loss: 0.5106 - AUC: 0.8657 - binary_accuracy: 0.7851 - val_loss: 0.4957 - val_AUC: 0.8782 - val_binary_accuracy: 0.8075\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 0s 475us/step - loss: 0.5033 - AUC: 0.8694 - binary_accuracy: 0.7866 - val_loss: 0.4900 - val_AUC: 0.8803 - val_binary_accuracy: 0.8094\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 0s 475us/step - loss: 0.4991 - AUC: 0.8714 - binary_accuracy: 0.7876 - val_loss: 0.4867 - val_AUC: 0.8818 - val_binary_accuracy: 0.8091\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 0s 471us/step - loss: 0.4964 - AUC: 0.8734 - binary_accuracy: 0.7883 - val_loss: 0.4845 - val_AUC: 0.8832 - val_binary_accuracy: 0.8087\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.7287 - AUC: 0.6167 - binary_accuracy: 0.5762 - val_loss: 0.6544 - val_AUC: 0.7629 - val_binary_accuracy: 0.6916\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 0s 475us/step - loss: 0.6087 - AUC: 0.8087 - binary_accuracy: 0.7345 - val_loss: 0.5614 - val_AUC: 0.8453 - val_binary_accuracy: 0.7663\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 0s 471us/step - loss: 0.5445 - AUC: 0.8467 - binary_accuracy: 0.7682 - val_loss: 0.5161 - val_AUC: 0.8633 - val_binary_accuracy: 0.7897\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 0s 485us/step - loss: 0.5161 - AUC: 0.8596 - binary_accuracy: 0.7818 - val_loss: 0.4972 - val_AUC: 0.8712 - val_binary_accuracy: 0.7972\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 0s 482us/step - loss: 0.5035 - AUC: 0.8659 - binary_accuracy: 0.7870 - val_loss: 0.4879 - val_AUC: 0.8771 - val_binary_accuracy: 0.8025\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 0s 480us/step - loss: 0.4965 - AUC: 0.8705 - binary_accuracy: 0.7891 - val_loss: 0.4823 - val_AUC: 0.8812 - val_binary_accuracy: 0.8075\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 0s 481us/step - loss: 0.4911 - AUC: 0.8748 - binary_accuracy: 0.7916 - val_loss: 0.4774 - val_AUC: 0.8856 - val_binary_accuracy: 0.8138\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 0s 483us/step - loss: 0.4864 - AUC: 0.8789 - binary_accuracy: 0.7958 - val_loss: 0.4731 - val_AUC: 0.8904 - val_binary_accuracy: 0.8194\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 0s 487us/step - loss: 0.4818 - AUC: 0.8838 - binary_accuracy: 0.8016 - val_loss: 0.4687 - val_AUC: 0.8953 - val_binary_accuracy: 0.8231\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 0s 479us/step - loss: 0.4773 - AUC: 0.8888 - binary_accuracy: 0.8084 - val_loss: 0.4647 - val_AUC: 0.9001 - val_binary_accuracy: 0.8300\n"
     ]
    }
   ],
   "source": [
    "# hidden_sizes = [2, 5, 10, 20, 50]\n",
    "# l2_regs = [0.01, 0.001, 0.0001]\n",
    "# activations = ['relu', 'tanh']\n",
    "\n",
    "hidden_sizes = [2, 5]\n",
    "l2_regs = [0.01]\n",
    "activations = ['relu', 'tanh']\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "early_call = tf.keras.callbacks.EarlyStopping(monitor='val_AUC', mode='max', patience=10, restore_best_weights=True)\n",
    "\n",
    "for activation in activations:\n",
    "    for l2_reg in l2_regs:\n",
    "        for hidden_size in hidden_sizes:\n",
    "            if activation == 'relu':\n",
    "                activate = tf.keras.activations.relu\n",
    "            elif activation == 'tanh':\n",
    "                activate = tf.keras.activations.tanh\n",
    "\n",
    "            # Create Tensorboard Callback\n",
    "            param_string = 'act-{},l2-{},hs-{}'.format(activation, l2_reg, hidden_size)\n",
    "            log_dir = 'logs/binary_classification_test/' + param_string\n",
    "            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "            # Input layer\n",
    "            inputs = tf.keras.Input(shape=(X_train.shape[1]))\n",
    "\n",
    "            # Hidden layer with regularization and ReLU\n",
    "            hidden = tf.keras.layers.Dense(hidden_size, kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(inputs)\n",
    "            hidden = activate(hidden)\n",
    "\n",
    "            # Output layer with regularization and sigmoid\n",
    "            outputs = tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(hidden)\n",
    "            outputs = tf.keras.activations.sigmoid(outputs)\n",
    "\n",
    "            model = tf.keras.Model(inputs=inputs, outputs=outputs, name='RegularizedModel')\n",
    "\n",
    "            model.compile(\n",
    "                    optimizer=tf.optimizers.Adam(),\n",
    "                    loss=tf.losses.BinaryCrossentropy(),\n",
    "                    metrics=[tf.keras.metrics.AUC(name='AUC'), tf.keras.metrics.BinaryAccuracy()],\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                      validation_data=(X_dev, y_dev),\n",
    "                      callbacks=[early_call, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 63394), started 0:01:15 ago. (Use '!kill 63394' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5db886195984c616\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5db886195984c616\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
